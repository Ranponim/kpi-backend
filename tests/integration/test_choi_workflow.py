"""
Choi ì•Œê³ ë¦¬ì¦˜ ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸

ì´ ëª¨ë“ˆì€ PEGProcessingServiceë¥¼ í†µí•œ ì™„ì „í•œ Choi ì•Œê³ ë¦¬ì¦˜ ì›Œí¬í”Œë¡œìš°ë¥¼
ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ê²€ì¦í•©ë‹ˆë‹¤. 6ì¥ í•„í„°ë§, 4ì¥ ì´ìƒíƒì§€, 5ì¥ KPIë¶„ì„ì˜
ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì¢…í•©ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

Author: Choi Algorithm Test Team
Created: 2025-09-20
"""

import pytest
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from app.services.peg_processing_service import PEGProcessingService
from app.models.judgement import PegSampleSeries, ChoiAlgorithmResponse

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TestChoiWorkflow:
    """
    Choi ì•Œê³ ë¦¬ì¦˜ ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤
    
    SOLID ì›ì¹™ì„ ì¤€ìˆ˜í•˜ì—¬ êµ¬í˜„ëœ Choi ì•Œê³ ë¦¬ì¦˜ ì‹œìŠ¤í…œì˜
    ì™„ì „í•œ í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    @pytest.fixture(scope="class")
    def peg_processing_service(self):
        """
        PEGProcessingService í”½ìŠ¤ì²˜ (í´ë˜ìŠ¤ ìŠ¤ì½”í”„)
        
        Strategy Factoryë¥¼ í†µí•œ ì™„ì „í•œ ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ìƒì„±
        """
        logger.info("Creating PEGProcessingService with Strategy Factory DI")
        service = PEGProcessingService()
        
        # ì„œë¹„ìŠ¤ ì •ìƒ ì´ˆê¸°í™” ê²€ì¦
        assert service.filtering_strategy is not None, "Filtering strategy should be initialized"
        assert service.judgement_strategy is not None, "Judgement strategy should be initialized"
        assert service.config is not None, "Configuration should be loaded"
        
        logger.info("PEGProcessingService fixture created successfully")
        return service
    
    @pytest.fixture(scope="class")
    def test_fixtures(self):
        """í…ŒìŠ¤íŠ¸ í”½ìŠ¤ì²˜ ë¡œë“œ"""
        fixtures_dir = Path(__file__).parent.parent / "fixtures" / "choi_algorithm"
        expected_outputs_dir = fixtures_dir / "expected_outputs"
        
        fixtures = {}
        
        # ì‹œë‚˜ë¦¬ì˜¤ í”½ìŠ¤ì²˜ ë¡œë“œ
        for fixture_file in fixtures_dir.glob("scenario_*.json"):
            with open(fixture_file, 'r', encoding='utf-8') as f:
                scenario_data = json.load(f)
                scenario_name = scenario_data["scenario_name"]  # JSON ë‚´ë¶€ì˜ scenario_name ì‚¬ìš©
                fixtures[scenario_name] = scenario_data
        
        # ì˜ˆìƒ ì¶œë ¥ ë¡œë“œ
        for expected_file in expected_outputs_dir.glob("*_expected.json"):
            scenario_name = expected_file.stem.replace("_expected", "")
            
            with open(expected_file, 'r', encoding='utf-8') as f:
                if scenario_name in fixtures:
                    fixtures[scenario_name]["expected_output"] = json.load(f)
        
        logger.info(f"Loaded {len(fixtures)} test fixtures")
        return fixtures
    
    @pytest.mark.parametrize("scenario_name", [
        "normal_case",
        "anomaly_detection_case", 
        "fifty_percent_rule_trigger"
    ])
    def test_choi_algorithm_scenarios(self, peg_processing_service, test_fixtures, scenario_name):
        """
        Choi ì•Œê³ ë¦¬ì¦˜ ì‹œë‚˜ë¦¬ì˜¤ë³„ í†µí•© í…ŒìŠ¤íŠ¸
        
        ê° ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ê³  
        ì˜ˆìƒ ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ ì •í™•ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
        """
        logger.info(f"ğŸ§ª Testing scenario: {scenario_name}")
        
        # í”½ìŠ¤ì²˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        scenario_data = test_fixtures[scenario_name]
        expected_output = scenario_data.get("expected_output")
        
        assert expected_output is not None, f"Expected output not found for {scenario_name}"
        
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        input_data = scenario_data["input_data"]
        cell_ids = scenario_data["cell_ids"]
        time_range = self._convert_time_range(scenario_data["time_range"])
        
        # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
        start_time = datetime.now()
        
        # ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        logger.info(f"Executing Choi algorithm for {scenario_name}")
        actual_response = peg_processing_service.process_peg_data(
            input_data, cell_ids, time_range
        )
        
        # ì„±ëŠ¥ ê²€ì¦
        processing_time = (datetime.now() - start_time).total_seconds()
        assert processing_time < 5.0, f"Processing time {processing_time:.2f}s exceeds 5s limit"
        
        logger.info(f"âœ… Performance test passed: {processing_time:.3f}s < 5.0s")
        
        # ê²°ê³¼ êµ¬ì¡° ê²€ì¦
        self._validate_response_structure(actual_response)
        
        # ì£¼ìš” ê²°ê³¼ ë¹„êµ
        self._compare_filtering_results(actual_response.filtering, expected_output["filtering"])
        self._compare_abnormal_detection_results(actual_response.abnormal_detection, expected_output["abnormal_detection"])
        
        # ë©”íƒ€ë°ì´í„° ê²€ì¦
        assert actual_response.total_cells_analyzed == len(cell_ids)
        assert actual_response.algorithm_version == expected_output["algorithm_version"]
        
        logger.info(f"ğŸ‰ Scenario {scenario_name} test passed completely")
    
    def test_choi_algorithm_performance_benchmark(self, peg_processing_service):
        """
        Choi ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
        
        ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¡œ ì„±ëŠ¥ ê¸°ì¤€ ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
        """
        logger.info("ğŸš€ Performance benchmark test")
        
        # ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (10ì…€ Ã— 5PEG Ã— 100ìƒ˜í”Œ)
        large_input_data = {"ems_ip": "192.168.1.200"}
        large_cell_ids = [f"cell_{i:03d}" for i in range(10)]
        time_range = {"start": datetime.now()}
        
        # ì„±ëŠ¥ ì¸¡ì •
        start_time = datetime.now()
        
        response = peg_processing_service.process_peg_data(
            large_input_data, large_cell_ids, time_range
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦ (PRD 4.3 ìš”êµ¬ì‚¬í•­)
        max_allowed_time = len(large_cell_ids) * 0.1  # ì…€ë‹¹ 100ms
        assert processing_time < max_allowed_time, f"Performance {processing_time:.2f}s exceeds {max_allowed_time:.2f}s"
        
        # ê²°ê³¼ ë¬´ê²°ì„± ê²€ì¦
        assert response.total_cells_analyzed == len(large_cell_ids)
        assert len(response.filtering.valid_time_slots) == len(large_cell_ids)
        
        logger.info(f"âœ… Performance benchmark passed: {processing_time:.3f}s for {len(large_cell_ids)} cells")
    
    def test_choi_algorithm_error_handling(self, peg_processing_service):
        """
        Choi ì•Œê³ ë¦¬ì¦˜ ì˜¤ë¥˜ ì²˜ë¦¬ ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸
        
        ì˜ëª»ëœ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ ì‹œìŠ¤í…œì˜ ê²¬ê³ ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
        """
        logger.info("ğŸ›¡ï¸ Error handling robustness test")
        
        # ë¹ˆ ë°ì´í„° í…ŒìŠ¤íŠ¸ - ì˜ˆì™¸ ë°œìƒ ì˜ˆìƒ
        with pytest.raises(RuntimeError, match="PEG processing failed"):
            peg_processing_service.process_peg_data(
                {"ems_ip": "test"}, [], {"start": datetime.now()}
            )
        
        logger.info("âœ… Empty data error handling verified")
        
        # ìµœì†Œí•œì˜ ìœ íš¨ ë°ì´í„°ë¡œ ì •ìƒ ë™ì‘ í™•ì¸
        minimal_response = peg_processing_service.process_peg_data(
            {"ems_ip": "test"}, ["cell_test"], {"start": datetime.now()}
        )
        
        # ìµœì†Œ ë°ì´í„°ë¡œë„ ì‘ë‹µ ìƒì„± í™•ì¸
        assert isinstance(minimal_response, ChoiAlgorithmResponse)
        assert minimal_response.total_cells_analyzed >= 0
        
        logger.info("âœ… Minimal data handling test passed")
        logger.info("âœ… Error handling robustness verified")
    
    def _convert_time_range(self, time_range_data: Dict[str, str]) -> Dict[str, datetime]:
        """ì‹œê°„ ë²”ìœ„ ë°ì´í„° ë³€í™˜"""
        return {
            key: datetime.fromisoformat(value) if isinstance(value, str) else value
            for key, value in time_range_data.items()
        }
    
    def _validate_response_structure(self, response: ChoiAlgorithmResponse) -> None:
        """ì‘ë‹µ êµ¬ì¡° ê²€ì¦"""
        # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ í™•ì¸
        assert hasattr(response, 'timestamp'), "Response should have timestamp"
        assert hasattr(response, 'processing_time_ms'), "Response should have processing_time_ms"
        assert hasattr(response, 'algorithm_version'), "Response should have algorithm_version"
        assert hasattr(response, 'filtering'), "Response should have filtering results"
        assert hasattr(response, 'abnormal_detection'), "Response should have abnormal_detection results"
        assert hasattr(response, 'kpi_judgement'), "Response should have kpi_judgement results"
        
        # íƒ€ì… ê²€ì¦
        assert isinstance(response.total_cells_analyzed, int), "total_cells_analyzed should be int"
        assert isinstance(response.total_pegs_analyzed, int), "total_pegs_analyzed should be int"
        assert isinstance(response.processing_warnings, list), "processing_warnings should be list"
        
        logger.debug("Response structure validation passed")
    
    def _compare_filtering_results(self, actual, expected) -> None:
        """í•„í„°ë§ ê²°ê³¼ ë¹„êµ"""
        # í•„í„°ë§ ë¹„ìœ¨ ê²€ì¦ (Â±5% í—ˆìš© ì˜¤ì°¨)
        expected_ratio_range = expected.get("filter_ratio_range", [0.0, 1.0])
        assert expected_ratio_range[0] <= actual.filter_ratio <= expected_ratio_range[1], \
            f"Filter ratio {actual.filter_ratio} not in expected range {expected_ratio_range}"
        
        # 50% ê·œì¹™ íŠ¸ë¦¬ê±° ê²€ì¦
        should_trigger = expected.get("should_trigger_50_percent_rule", False)
        has_warning = actual.warning_message is not None
        assert has_warning == should_trigger, \
            f"50% rule trigger mismatch: expected {should_trigger}, got {has_warning}"
        
        logger.debug("Filtering results comparison passed")
    
    def _compare_abnormal_detection_results(self, actual, expected) -> None:
        """ì´ìƒ íƒì§€ ê²°ê³¼ ë¹„êµ"""
        expected_anomalies = set(expected.get("expected_anomalies", []))
        expected_displays = expected.get("display_results", {})
        
        # Î±0 ê·œì¹™ì— ë”°ë¥¸ í‘œì‹œ ê²°ê³¼ ê²€ì¦
        for anomaly_type, should_display in expected_displays.items():
            actual_display = actual.display_results.get(anomaly_type, False)
            assert actual_display == should_display, \
                f"Display result for {anomaly_type}: expected {should_display}, got {actual_display}"
        
        logger.debug("Abnormal detection results comparison passed")


# =============================================================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜ë“¤
# =============================================================================

def run_single_scenario_test(scenario_name: str) -> bool:
    """
    ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        scenario_name: í…ŒìŠ¤íŠ¸í•  ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„
        
    Returns:
        bool: í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€
    """
    try:
        logger.info(f"Running single scenario test: {scenario_name}")
        
        # ì„œë¹„ìŠ¤ ìƒì„±
        service = PEGProcessingService()
        
        # í”½ìŠ¤ì²˜ ë¡œë“œ
        fixtures_dir = Path(__file__).parent.parent / "fixtures" / "choi_algorithm"
        fixture_file = fixtures_dir / f"scenario_{scenario_name}.json"
        
        if not fixture_file.exists():
            logger.error(f"Fixture file not found: {fixture_file}")
            return False
        
        with open(fixture_file, 'r', encoding='utf-8') as f:
            scenario_data = json.load(f)
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        input_data = scenario_data["input_data"]
        cell_ids = scenario_data["cell_ids"]
        time_range = {
            key: datetime.fromisoformat(value) if isinstance(value, str) else value
            for key, value in scenario_data["time_range"].items()
        }
        
        response = service.process_peg_data(input_data, cell_ids, time_range)
        
        # ê¸°ë³¸ ê²€ì¦
        assert isinstance(response, ChoiAlgorithmResponse)
        assert response.total_cells_analyzed == len(cell_ids)
        
        logger.info(f"âœ… Single scenario test {scenario_name} passed")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Single scenario test {scenario_name} failed: {e}")
        return False


def run_all_integration_tests() -> bool:
    """
    ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Returns:
        bool: ì „ì²´ í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€
    """
    try:
        logger.info("ğŸ§ª Running all Choi algorithm integration tests")
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸
        scenarios = ["normal", "anomalies", "fifty_percent_rule"]
        
        results = []
        for scenario in scenarios:
            result = run_single_scenario_test(scenario)
            results.append(result)
            
            if result:
                logger.info(f"âœ… {scenario} scenario: PASSED")
            else:
                logger.error(f"âŒ {scenario} scenario: FAILED")
        
        # ì „ì²´ ê²°ê³¼
        all_passed = all(results)
        
        if all_passed:
            logger.info("ğŸ‰ All integration tests PASSED")
        else:
            failed_count = sum(1 for r in results if not r)
            logger.error(f"âŒ {failed_count}/{len(results)} tests FAILED")
        
        return all_passed
        
    except Exception as e:
        logger.error(f"Error running integration tests: {e}")
        return False


# =============================================================================
# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
# =============================================================================

class TestChoiPerformance:
    """Choi ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    @pytest.fixture
    def performance_service(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ì„œë¹„ìŠ¤"""
        return PEGProcessingService()
    
    @pytest.mark.performance
    def test_large_dataset_performance(self, performance_service):
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸš€ Large dataset performance test")
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° (50ì…€ Ã— 10PEG)
        large_cell_ids = [f"cell_{i:03d}" for i in range(50)]
        input_data = {"ems_ip": "192.168.1.200"}
        time_range = {"start": datetime.now()}
        
        start_time = datetime.now()
        
        response = performance_service.process_peg_data(
            input_data, large_cell_ids, time_range
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # ì„±ëŠ¥ ê¸°ì¤€: ì…€ë‹¹ 100ms ì´í•˜
        max_allowed = len(large_cell_ids) * 0.1
        assert processing_time < max_allowed, \
            f"Performance {processing_time:.2f}s exceeds limit {max_allowed:.2f}s"
        
        # ê²°ê³¼ ë¬´ê²°ì„±
        assert response.total_cells_analyzed == len(large_cell_ids)
        
        logger.info(f"âœ… Performance test passed: {processing_time:.3f}s for {len(large_cell_ids)} cells")
    
    @pytest.mark.performance  
    def test_memory_efficiency(self, performance_service):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸"""
        import psutil
        import os
        
        logger.info("ğŸ’¾ Memory efficiency test")
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # ì—°ì†ì ì¸ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸
        for i in range(10):
            cell_ids = [f"cell_{j:03d}" for j in range(5)]
            response = performance_service.process_peg_data(
                {"ems_ip": f"192.168.1.{200+i}"}, cell_ids, {"start": datetime.now()}
            )
            assert isinstance(response, ChoiAlgorithmResponse)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ì´ 100MB ì´í•˜ì—¬ì•¼ í•¨
        assert memory_increase < 100, f"Memory increase {memory_increase:.1f}MB exceeds 100MB limit"
        
        logger.info(f"âœ… Memory efficiency test passed: {memory_increase:.1f}MB increase")


# =============================================================================
# ë©”ì¸ ì‹¤í–‰ (pytest ì—†ì´ ì§ì ‘ ì‹¤í–‰ìš©)
# =============================================================================

if __name__ == "__main__":
    print("ğŸ§ª Choi ì•Œê³ ë¦¬ì¦˜ í†µí•© í…ŒìŠ¤íŠ¸ ì§ì ‘ ì‹¤í–‰")
    print("=" * 50)
    
    try:
        # ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        success = run_all_integration_tests()
        
        if success:
            print("ğŸ‰ ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            print("ğŸ† Choi ì•Œê³ ë¦¬ì¦˜ ì‹œìŠ¤í…œ ì™„ì „ ê²€ì¦ ì™„ë£Œ!")
        else:
            print("âŒ ì¼ë¶€ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
