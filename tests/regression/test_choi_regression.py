"""
Choi ì•Œê³ ë¦¬ì¦˜ íšŒê·€ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬

ì´ ëª¨ë“ˆì€ ê³ ì •ëœ ì…ë ¥/ì¶œë ¥ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ Choi ì•Œê³ ë¦¬ì¦˜ì˜
ì˜ë„í•˜ì§€ ì•Šì€ ë³€ê²½ì„ ê°ì§€í•˜ëŠ” íšŒê·€ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ëª¨ë“  í…ŒìŠ¤íŠ¸ëŠ” ê¸°ì¡´ì— ê²€ì¦ëœ 'ê³¨ë“ ' ì¶œë ¥ê³¼ ë¹„êµí•˜ì—¬
ì•Œê³ ë¦¬ì¦˜ ë¡œì§ì˜ ì¼ê´€ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

Author: Choi Algorithm Regression Test Team
Created: 2025-09-20
"""

import pytest
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple
import sys
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from app.services.peg_processing_service import PEGProcessingService
from app.models.judgement import PegSampleSeries, ChoiAlgorithmResponse

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.WARNING)  # íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹œ ë¡œê·¸ ìµœì†Œí™”
logger = logging.getLogger(__name__)


class TestChoiRegression:
    """
    Choi ì•Œê³ ë¦¬ì¦˜ íšŒê·€ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤
    
    ê³ ì •ëœ ì…ë ¥/ì¶œë ¥ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ê´€ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    ëª¨ë“  í…ŒìŠ¤íŠ¸ëŠ” ì´ì „ì— ê²€ì¦ëœ ê³¨ë“  ì¶œë ¥ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    
    @pytest.fixture(scope="class")
    def regression_service(self):
        """íšŒê·€ í…ŒìŠ¤íŠ¸ìš© PEGProcessingService í”½ìŠ¤ì²˜"""
        logger.info("Creating PEGProcessingService for regression testing")
        service = PEGProcessingService()
        
        # ì„œë¹„ìŠ¤ ì •ìƒ ì´ˆê¸°í™” ê²€ì¦
        assert service.filtering_strategy is not None
        assert service.judgement_strategy is not None
        assert service.config is not None
        
        return service
    
    @pytest.fixture(scope="class")
    def regression_fixtures(self):
        """íšŒê·€ í…ŒìŠ¤íŠ¸ í”½ìŠ¤ì²˜ ë¡œë“œ"""
        data_dir = Path(__file__).parent / "data"
        fixtures = {}
        
        # ëª¨ë“  ì…ë ¥ íŒŒì¼ ê²€ìƒ‰
        input_files = list(data_dir.rglob("*_input.json"))
        
        for input_file in input_files:
            # ëŒ€ì‘í•˜ëŠ” ì˜ˆìƒ ì¶œë ¥ íŒŒì¼ ì°¾ê¸°
            expected_file = input_file.parent / input_file.name.replace("_input.json", "_expected.json")
            
            if expected_file.exists():
                scenario_name = input_file.stem.replace("_input", "")
                
                # ì…ë ¥ ë°ì´í„° ë¡œë“œ
                with open(input_file, 'r', encoding='utf-8') as f:
                    input_data = json.load(f)
                
                # ì˜ˆìƒ ì¶œë ¥ ë¡œë“œ
                with open(expected_file, 'r', encoding='utf-8') as f:
                    expected_output = json.load(f)
                
                fixtures[scenario_name] = {
                    "input": input_data,
                    "expected": expected_output,
                    "category": input_file.parent.name,
                    "input_file": input_file.name,
                    "expected_file": expected_file.name
                }
        
        logger.info(f"Loaded {len(fixtures)} regression test fixtures")
        return fixtures
    
    @pytest.mark.regression
    @pytest.mark.parametrize("scenario_name", [
        # ìë™ìœ¼ë¡œ ë°œê²¬ëœ ì‹œë‚˜ë¦¬ì˜¤ë“¤ì´ ì—¬ê¸°ì— ì¶”ê°€ë©ë‹ˆë‹¤
        "basic_similar_judgement",
        "nd_cant_judge", 
        "exactly_50_percent_filter",
        "zero_samples_handling",
        "extreme_cv_values",
        "improve_degrade_detection",
        "beta_threshold_boundaries",
        "high_delta_anomaly"
    ])
    def test_choi_algorithm_regression(self, regression_service, regression_fixtures, scenario_name):
        """
        Choi ì•Œê³ ë¦¬ì¦˜ íšŒê·€ í…ŒìŠ¤íŠ¸
        
        ê° ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ê³ ì •ëœ ì…ë ¥ìœ¼ë¡œ ì•Œê³ ë¦¬ì¦˜ì„ ì‹¤í–‰í•˜ê³ 
        ì´ì „ì— ê²€ì¦ëœ ê³¨ë“  ì¶œë ¥ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        """
        logger.info(f"ğŸ”„ Regression test: {scenario_name}")
        
        # í”½ìŠ¤ì²˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        fixture_data = regression_fixtures.get(scenario_name)
        assert fixture_data is not None, f"Fixture not found for scenario: {scenario_name}"
        
        input_data = fixture_data["input"]
        expected_output = fixture_data["expected"]
        
        # ì…ë ¥ ë°ì´í„° ë³€í™˜
        converted_input = self._convert_input_data(input_data)
        
        # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
        start_time = time.time()
        
        # ì‹¤ì œ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
        actual_response = self._execute_algorithm_with_test_data(
            regression_service, converted_input
        )
        
        # ì„±ëŠ¥ ê²€ì¦
        processing_time = (time.time() - start_time) * 1000
        expected_max_time = input_data["expected_behavior"].get("performance_max_ms", 5000)
        
        assert processing_time < expected_max_time, \
            f"Performance regression: {processing_time:.2f}ms > {expected_max_time}ms"
        
        # ì‘ë‹µì„ JSON í˜•íƒœë¡œ ë³€í™˜
        actual_output = self._serialize_response(actual_response)
        
        # ì£¼ìš” ê²°ê³¼ ë¹„êµ (íƒ€ì„ìŠ¤íƒ¬í”„ ë“± ë³€ë™ í•„ë“œ ì œì™¸)
        self._compare_regression_outputs(actual_output, expected_output, scenario_name)
        
        logger.info(f"âœ… Regression test {scenario_name} passed: {processing_time:.2f}ms")
    
    @pytest.mark.regression
    @pytest.mark.performance
    def test_performance_regression_baseline(self, regression_service):
        """
        ì„±ëŠ¥ íšŒê·€ í…ŒìŠ¤íŠ¸ ê¸°ì¤€ì„ 
        
        í‘œì¤€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ ê¸°ì¤€ì„ ì„ ì„¤ì •í•˜ê³ 
        í–¥í›„ ì„±ëŠ¥ íšŒê·€ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
        """
        logger.info("ğŸš€ Performance regression baseline test")
        
        # í‘œì¤€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        standard_cell_ids = [f"perf_cell_{i:03d}" for i in range(10)]
        input_data = {"ems_ip": "192.168.200.1"}
        time_range = {"start": datetime.now()}
        
        # ì„±ëŠ¥ ì¸¡ì •
        start_time = time.time()
        
        response = regression_service.process_peg_data(
            input_data, standard_cell_ids, time_range
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        # ì„±ëŠ¥ ê¸°ì¤€ì„ : 10ì…€ Ã— 50ms = 500ms ì´í•˜
        baseline_max_time = 500.0
        assert processing_time < baseline_max_time, \
            f"Performance regression: {processing_time:.2f}ms > {baseline_max_time}ms"
        
        # ê²°ê³¼ ë¬´ê²°ì„± ê²€ì¦
        assert response.total_cells_analyzed == len(standard_cell_ids)
        assert isinstance(response, ChoiAlgorithmResponse)
        
        logger.info(f"âœ… Performance baseline: {processing_time:.2f}ms < {baseline_max_time}ms")
    
    def _convert_input_data(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ë³€í™˜"""
        return {
            "input_data": input_data["input_data"],
            "cell_ids": input_data["cell_ids"],
            "time_range": {
                key: datetime.fromisoformat(value) if isinstance(value, str) else value
                for key, value in input_data["time_range"].items()
            },
            "peg_data": self._convert_peg_data(input_data["peg_data"])
        }
    
    def _convert_peg_data(self, peg_data: Dict[str, List[Dict[str, Any]]]) -> Dict[str, List[PegSampleSeries]]:
        """PEG ë°ì´í„°ë¥¼ PegSampleSeriesë¡œ ë³€í™˜"""
        converted_data = {}
        
        for cell_id, peg_list in peg_data.items():
            series_list = []
            
            for peg_info in peg_list:
                # null ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                pre_samples = [None if x is None else x for x in peg_info["pre_samples"]]
                post_samples = [None if x is None else x for x in peg_info["post_samples"]]
                
                series = PegSampleSeries(
                    peg_name=peg_info["peg_name"],
                    cell_id=peg_info["cell_id"],
                    pre_samples=pre_samples,
                    post_samples=post_samples,
                    unit=peg_info["unit"]
                )
                
                series_list.append(series)
            
            converted_data[cell_id] = series_list
        
        return converted_data
    
    def _execute_algorithm_with_test_data(self, 
                                        service: PEGProcessingService,
                                        converted_input: Dict[str, Any]) -> ChoiAlgorithmResponse:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰"""
        # ì„œë¹„ìŠ¤ì˜ ë‚´ë¶€ ë©”ì„œë“œë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì£¼ì…
        validated_data = converted_input["peg_data"]
        
        # í•„í„°ë§ ì‹¤í–‰
        filtering_result = service._run_filtering(validated_data)
        
        # ì§‘ê³„ ë° íŒŒìƒ ê³„ì‚°
        aggregated_data = service._aggregation(validated_data, filtering_result)
        derived_data = service._derived_calculation(aggregated_data)
        
        # íŒì • ì‹¤í–‰
        judgement_result = service._run_judgement(derived_data, filtering_result)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        processing_results = {
            "filtering": filtering_result,
            "aggregation": aggregated_data,
            "derived_calculation": derived_data,
            "judgement": judgement_result
        }
        
        warnings = []
        if filtering_result.warning_message:
            warnings.append(filtering_result.warning_message)
        
        return service._result_formatting(processing_results, warnings)
    
    def _serialize_response(self, response: ChoiAlgorithmResponse) -> Dict[str, Any]:
        """ì‘ë‹µì„ JSON í˜•íƒœë¡œ ì§ë ¬í™”"""
        if hasattr(response, 'model_dump'):
            return response.model_dump()
        else:
            return response.__dict__
    
    def _compare_regression_outputs(self, 
                                  actual: Dict[str, Any], 
                                  expected: Dict[str, Any], 
                                  scenario_name: str) -> None:
        """íšŒê·€ í…ŒìŠ¤íŠ¸ ì¶œë ¥ ë¹„êµ"""
        # ë³€ë™ ê°€ëŠ¥í•œ í•„ë“œë“¤ ì œì™¸í•˜ê³  ë¹„êµ
        differences = self._find_significant_differences(actual, expected)
        
        if differences:
            # ì°¨ì´ì ì´ ìˆìœ¼ë©´ ìƒì„¸ ì •ë³´ ì¶œë ¥
            self._report_regression_failure(differences, scenario_name, actual, expected)
            pytest.fail(f"Regression test failed for {scenario_name}: {differences}")
        
        # í•µì‹¬ í•„ë“œë“¤ì˜ êµ¬ì²´ì  ê²€ì¦
        self._verify_core_algorithm_outputs(actual, expected, scenario_name)
    
    def _find_significant_differences(self, 
                                    actual: Dict[str, Any], 
                                    expected: Dict[str, Any]) -> List[str]:
        """ì¤‘ìš”í•œ ì°¨ì´ì  ì°¾ê¸° (ë‚´ì¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš©)"""
        differences = []
        
        # ë³€ë™ ê°€ëŠ¥í•œ í•„ë“œë“¤ (ë¹„êµ ì œì™¸)
        excluded_fields = {"timestamp", "processing_time_ms", "regression_metadata"}
        
        # ì¬ê·€ì ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ ë¹„êµ
        def compare_dicts(actual_dict, expected_dict, path=""):
            for key in expected_dict:
                if key in excluded_fields:
                    continue
                    
                current_path = f"{path}.{key}" if path else key
                
                if key not in actual_dict:
                    differences.append(f"Missing key: {current_path}")
                    continue
                
                actual_val = actual_dict[key]
                expected_val = expected_dict[key]
                
                if isinstance(expected_val, dict) and isinstance(actual_val, dict):
                    compare_dicts(actual_val, expected_val, current_path)
                elif isinstance(expected_val, list) and isinstance(actual_val, list):
                    if len(actual_val) != len(expected_val):
                        differences.append(f"List length mismatch at {current_path}: {len(actual_val)} vs {len(expected_val)}")
                    else:
                        for i, (a_item, e_item) in enumerate(zip(actual_val, expected_val)):
                            if isinstance(e_item, dict) and isinstance(a_item, dict):
                                compare_dicts(a_item, e_item, f"{current_path}[{i}]")
                            elif a_item != e_item:
                                differences.append(f"List item mismatch at {current_path}[{i}]: {a_item} vs {e_item}")
                elif isinstance(expected_val, float) and isinstance(actual_val, float):
                    # ë¶€ë™ì†Œìˆ˜ì  ë¹„êµ (Â±0.001 í—ˆìš©)
                    if abs(actual_val - expected_val) > 0.001:
                        differences.append(f"Float mismatch at {current_path}: {actual_val} vs {expected_val}")
                elif actual_val != expected_val:
                    differences.append(f"Value mismatch at {current_path}: {actual_val} vs {expected_val}")
        
        compare_dicts(actual, expected)
        return differences
    
    def _verify_core_algorithm_outputs(self, 
                                     actual: Dict[str, Any], 
                                     expected: Dict[str, Any], 
                                     scenario_name: str) -> None:
        """í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ì¶œë ¥ ê²€ì¦"""
        # í•„í„°ë§ ê²°ê³¼ ê²€ì¦
        actual_filter_ratio = actual["filtering"]["filter_ratio"]
        expected_filter_ratio = expected["filtering"]["filter_ratio"]
        
        assert abs(actual_filter_ratio - expected_filter_ratio) < 0.01, \
            f"{scenario_name}: Filter ratio mismatch {actual_filter_ratio} != {expected_filter_ratio}"
        
        # ì´ìƒ íƒì§€ ê²°ê³¼ ê²€ì¦
        actual_display = actual["abnormal_detection"]["display_results"]
        expected_display = expected["abnormal_detection"]["display_results"]
        
        for anomaly_type in ["Range", "ND", "Zero", "New", "High Delta"]:
            actual_val = actual_display.get(anomaly_type, False)
            expected_val = expected_display.get(anomaly_type, False)
            
            assert actual_val == expected_val, \
                f"{scenario_name}: Anomaly display mismatch for {anomaly_type}: {actual_val} != {expected_val}"
        
        # ì•Œê³ ë¦¬ì¦˜ ë²„ì „ ê²€ì¦
        assert actual["algorithm_version"] == expected["algorithm_version"], \
            f"{scenario_name}: Algorithm version mismatch"
    
    def _report_regression_failure(self, 
                                 differences: List[str], 
                                 scenario_name: str, 
                                 actual: Dict[str, Any], 
                                 expected: Dict[str, Any]) -> None:
        """íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ìƒì„¸ ë³´ê³ """
        print(f"\nâŒ REGRESSION TEST FAILURE: {scenario_name}")
        print("=" * 60)
        print("ğŸ” ìƒì„¸ ì°¨ì´ì :")
        for diff in differences:
            print(f"  - {diff}")
        
        # í•µì‹¬ í•„ë“œë³„ ë¹„êµ
        print("\nğŸ“Š í•µì‹¬ ê²°ê³¼ ë¹„êµ:")
        
        # í•„í„°ë§ ë¹„êµ
        actual_filter = actual.get("filtering", {})
        expected_filter = expected.get("filtering", {})
        print(f"  í•„í„°ë§ ë¹„ìœ¨: {actual_filter.get('filter_ratio')} vs {expected_filter.get('filter_ratio')}")
        
        # ì´ìƒ íƒì§€ ë¹„êµ
        actual_anomaly = actual.get("abnormal_detection", {}).get("display_results", {})
        expected_anomaly = expected.get("abnormal_detection", {}).get("display_results", {})
        print(f"  ì´ìƒ íƒì§€: {actual_anomaly} vs {expected_anomaly}")
        
        print("=" * 60)


# =============================================================================
# íšŒê·€ í…ŒìŠ¤íŠ¸ ìë™ ë°œê²¬ ë° ì‹¤í–‰
# =============================================================================

class RegressionTestDiscovery:
    """íšŒê·€ í…ŒìŠ¤íŠ¸ ìë™ ë°œê²¬ê¸°"""
    
    @staticmethod
    def discover_regression_scenarios() -> List[str]:
        """íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ìë™ ë°œê²¬"""
        data_dir = Path(__file__).parent / "data"
        input_files = list(data_dir.rglob("*_input.json"))
        
        scenarios = []
        for input_file in input_files:
            expected_file = input_file.parent / input_file.name.replace("_input.json", "_expected.json")
            if expected_file.exists():
                scenario_name = input_file.stem.replace("_input", "")
                scenarios.append(scenario_name)
        
        return sorted(scenarios)
    
    @staticmethod
    def generate_pytest_parametrize_list() -> str:
        """pytest.mark.parametrizeìš© ì‹œë‚˜ë¦¬ì˜¤ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        scenarios = RegressionTestDiscovery.discover_regression_scenarios()
        return '["' + '", "'.join(scenarios) + '"]'


# =============================================================================
# íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ë³´ê³ 
# =============================================================================

class RegressionTestRunner:
    """íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        """ì‹¤í–‰ê¸° ì´ˆê¸°í™”"""
        self.service = PEGProcessingService()
        self.data_dir = Path(__file__).parent / "data"
        
    def run_all_regression_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘"""
        scenarios = RegressionTestDiscovery.discover_regression_scenarios()
        
        results = {
            "total_scenarios": len(scenarios),
            "passed": 0,
            "failed": 0,
            "failures": [],
            "performance_stats": [],
            "start_time": datetime.now().isoformat()
        }
        
        print(f"ğŸ”„ íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {len(scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
        
        for scenario_name in scenarios:
            try:
                performance_ms = self._run_single_regression_test(scenario_name)
                results["passed"] += 1
                results["performance_stats"].append({
                    "scenario": scenario_name,
                    "time_ms": performance_ms
                })
                print(f"  âœ… {scenario_name}: {performance_ms:.2f}ms")
                
            except Exception as e:
                results["failed"] += 1
                results["failures"].append({
                    "scenario": scenario_name,
                    "error": str(e)
                })
                print(f"  âŒ {scenario_name}: {e}")
        
        results["end_time"] = datetime.now().isoformat()
        return results
    
    def _run_single_regression_test(self, scenario_name: str) -> float:
        """ë‹¨ì¼ íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        # ì…ë ¥ ë° ì˜ˆìƒ ì¶œë ¥ ë¡œë“œ
        input_file = None
        expected_file = None
        
        for input_path in self.data_dir.rglob(f"{scenario_name}_input.json"):
            input_file = input_path
            expected_file = input_path.parent / f"{scenario_name}_expected.json"
            break
        
        if not input_file or not expected_file.exists():
            raise FileNotFoundError(f"Fixture files not found for {scenario_name}")
        
        with open(input_file, 'r', encoding='utf-8') as f:
            input_data = json.load(f)
        
        with open(expected_file, 'r', encoding='utf-8') as f:
            expected_output = json.load(f)
        
        # ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
        start_time = time.time()
        
        converted_input = self._convert_input_data(input_data)
        actual_response = self._execute_algorithm_with_test_data(converted_input)
        
        processing_time = (time.time() - start_time) * 1000
        
        # ê²°ê³¼ ë¹„êµ
        actual_output = self._serialize_response(actual_response)
        self._verify_regression_match(actual_output, expected_output)
        
        return processing_time
    
    def _convert_input_data(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ë³€í™˜ (ê³µí†µ ë¡œì§)"""
        return {
            "input_data": input_data["input_data"],
            "cell_ids": input_data["cell_ids"],
            "time_range": {
                key: datetime.fromisoformat(value) if isinstance(value, str) else value
                for key, value in input_data["time_range"].items()
            },
            "peg_data": self._convert_peg_data(input_data["peg_data"])
        }
    
    def _convert_peg_data(self, peg_data: Dict[str, List[Dict[str, Any]]]) -> Dict[str, List[PegSampleSeries]]:
        """PEG ë°ì´í„° ë³€í™˜ (ê³µí†µ ë¡œì§)"""
        converted_data = {}
        
        for cell_id, peg_list in peg_data.items():
            series_list = []
            
            for peg_info in peg_list:
                pre_samples = [None if x is None else x for x in peg_info["pre_samples"]]
                post_samples = [None if x is None else x for x in peg_info["post_samples"]]
                
                series = PegSampleSeries(
                    peg_name=peg_info["peg_name"],
                    cell_id=peg_info["cell_id"],
                    pre_samples=pre_samples,
                    post_samples=post_samples,
                    unit=peg_info["unit"]
                )
                
                series_list.append(series)
            
            converted_data[cell_id] = series_list
        
        return converted_data
    
    def _execute_algorithm_with_test_data(self, converted_input: Dict[str, Any]) -> ChoiAlgorithmResponse:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ (ê³µí†µ ë¡œì§)"""
        validated_data = converted_input["peg_data"]
        
        # ë‹¨ê³„ë³„ ì‹¤í–‰
        filtering_result = self.service._run_filtering(validated_data)
        aggregated_data = self.service._aggregation(validated_data, filtering_result)
        derived_data = self.service._derived_calculation(aggregated_data)
        judgement_result = self.service._run_judgement(derived_data, filtering_result)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        processing_results = {
            "filtering": filtering_result,
            "aggregation": aggregated_data,
            "derived_calculation": derived_data,
            "judgement": judgement_result
        }
        
        warnings = []
        if filtering_result.warning_message:
            warnings.append(filtering_result.warning_message)
        
        return self.service._result_formatting(processing_results, warnings)
    
    def _serialize_response(self, response: ChoiAlgorithmResponse) -> Dict[str, Any]:
        """ì‘ë‹µ ì§ë ¬í™” (ê³µí†µ ë¡œì§)"""
        if hasattr(response, 'model_dump'):
            return response.model_dump()
        else:
            return response.__dict__
    
    def _verify_regression_match(self, actual: Dict[str, Any], expected: Dict[str, Any]) -> None:
        """íšŒê·€ ë§¤ì¹˜ ê²€ì¦ (ê³µí†µ ë¡œì§)"""
        # í•µì‹¬ í•„ë“œë§Œ ë¹„êµ
        core_fields = ["algorithm_version", "filtering", "abnormal_detection", "kpi_judgement"]
        
        for field in core_fields:
            if field in expected:
                assert field in actual, f"Missing field: {field}"
                
                if field == "filtering":
                    # í•„í„°ë§ ë¹„ìœ¨ë§Œ ë¹„êµ (Â±1% í—ˆìš©)
                    expected_ratio = expected[field]["filter_ratio"]
                    actual_ratio = actual[field]["filter_ratio"]
                    assert abs(actual_ratio - expected_ratio) < 0.01
                elif field == "abnormal_detection":
                    # í‘œì‹œ ê²°ê³¼ë§Œ ë¹„êµ
                    expected_display = expected[field]["display_results"]
                    actual_display = actual[field]["display_results"]
                    assert actual_display == expected_display


# =============================================================================
# ì§ì ‘ ì‹¤í–‰ (pytest ì—†ì´)
# =============================================================================

def run_regression_tests_directly():
    """íšŒê·€ í…ŒìŠ¤íŠ¸ ì§ì ‘ ì‹¤í–‰"""
    print("ğŸ”„ Choi ì•Œê³ ë¦¬ì¦˜ íšŒê·€ í…ŒìŠ¤íŠ¸ ì§ì ‘ ì‹¤í–‰")
    print("=" * 50)
    
    try:
        runner = RegressionTestRunner()
        results = runner.run_all_regression_tests()
        
        print(f"\nğŸ“Š íšŒê·€ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"  ì´ ì‹œë‚˜ë¦¬ì˜¤: {results['total_scenarios']}")
        print(f"  ì„±ê³µ: {results['passed']}")
        print(f"  ì‹¤íŒ¨: {results['failed']}")
        
        if results["failures"]:
            print(f"\nâŒ ì‹¤íŒ¨í•œ ì‹œë‚˜ë¦¬ì˜¤:")
            for failure in results["failures"]:
                print(f"  - {failure['scenario']}: {failure['error']}")
        
        # ì„±ëŠ¥ í†µê³„
        if results["performance_stats"]:
            times = [stat["time_ms"] for stat in results["performance_stats"]]
            avg_time = sum(times) / len(times)
            max_time = max(times)
            print(f"\nâ±ï¸ ì„±ëŠ¥ í†µê³„:")
            print(f"  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ms")
            print(f"  ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„: {max_time:.2f}ms")
        
        if results["failed"] == 0:
            print("\nğŸ‰ ëª¨ë“  íšŒê·€ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            print("ğŸ† ì•Œê³ ë¦¬ì¦˜ ì¼ê´€ì„± ì™„ì „ ê²€ì¦!")
            return True
        else:
            print(f"\nâŒ {results['failed']}ê°œ íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        print(f"âŒ íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_regression_tests_directly()
    if not success:
        sys.exit(1)
