"""
Choi ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸

ì´ ëª¨ë“ˆì€ Choi ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
PRD 4.3ì˜ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ (< 5ì´ˆ)ì„ ê²€ì¦í•˜ê³  ë³‘ëª© ì§€ì ì„ ì‹ë³„í•©ë‹ˆë‹¤.

Author: Choi Algorithm Performance Team
Created: 2025-09-20
"""

import time
import logging
import statistics
import psutil
import tracemalloc
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Tuple
import sys
import json
import cProfile
import pstats
import io

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.services.peg_processing_service import PEGProcessingService
from app.models.judgement import PegSampleSeries

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.WARNING)  # ë²¤ì¹˜ë§ˆí¬ ì‹œ ë¡œê·¸ ìµœì†Œí™”
logger = logging.getLogger(__name__)


class ChoiPerformanceBenchmark:
    """
    Choi ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    
    ë‹¤ì–‘í•œ ë°ì´í„° í¬ê¸°ì™€ ë³µì¡ë„ë¡œ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³ 
    PRD 4.3 ìš”êµ¬ì‚¬í•­ ëŒ€ë¹„ ì„±ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”"""
        self.service = PEGProcessingService()
        self.results = []
        self.baseline_results = {}
        
        # ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ (PRD 4.3)
        self.performance_targets = {
            "small_workload": 100,      # 1-2ì…€: 100ms
            "standard_workload": 5000,  # 10ì…€: 5ì´ˆ
            "large_workload": 15000     # 50ì…€: 15ì´ˆ
        }
        
        logger.info("Choi performance benchmark initialized")
    
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """í¬ê´„ì  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ Choi ì•Œê³ ë¦¬ì¦˜ í¬ê´„ì  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("=" * 60)
        
        try:
            # 1. ê¸°ë³¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
            print("1. ê¸°ë³¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:")
            basic_results = self._run_basic_performance_tests()
            
            # 2. í™•ì¥ì„± í…ŒìŠ¤íŠ¸
            print("\n2. í™•ì¥ì„± í…ŒìŠ¤íŠ¸:")
            scalability_results = self._run_scalability_tests()
            
            # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
            print("\n3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸:")
            memory_results = self._run_memory_tests()
            
            # 4. ë‹¨ê³„ë³„ ì„±ëŠ¥ ë¶„ì„
            print("\n4. ë‹¨ê³„ë³„ ì„±ëŠ¥ ë¶„ì„:")
            phase_results = self._run_phase_performance_analysis()
            
            # 5. ì„±ëŠ¥ íšŒê·€ ê¸°ì¤€ì„  ì„¤ì •
            print("\n5. ì„±ëŠ¥ íšŒê·€ ê¸°ì¤€ì„ :")
            baseline_results = self._establish_performance_baseline()
            
            # ì¢…í•© ê²°ê³¼
            comprehensive_results = {
                "timestamp": datetime.now().isoformat(),
                "basic_performance": basic_results,
                "scalability": scalability_results,
                "memory_usage": memory_results,
                "phase_analysis": phase_results,
                "baseline": baseline_results,
                "targets_met": self._verify_performance_targets(basic_results)
            }
            
            # ê²°ê³¼ ì €ì¥
            self._save_benchmark_results(comprehensive_results)
            
            # ìš”ì•½ ë³´ê³ 
            self._report_benchmark_summary(comprehensive_results)
            
            return comprehensive_results
            
        except Exception as e:
            logger.error(f"Benchmark execution failed: {e}")
            raise
    
    def _run_basic_performance_tests(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        test_cases = [
            {"name": "small_workload", "cells": 2, "iterations": 10},
            {"name": "standard_workload", "cells": 10, "iterations": 5}, 
            {"name": "large_workload", "cells": 50, "iterations": 3}
        ]
        
        results = {}
        
        for test_case in test_cases:
            print(f"  ğŸ”„ {test_case['name']}: {test_case['cells']}ì…€ Ã— {test_case['iterations']}íšŒ")
            
            times = []
            for i in range(test_case['iterations']):
                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
                cell_ids = [f"bench_cell_{j:03d}" for j in range(test_case['cells'])]
                input_data = {"ems_ip": "192.168.200.1"}
                time_range = {"start": datetime.now()}
                
                # ì„±ëŠ¥ ì¸¡ì •
                start_time = time.perf_counter()
                
                response = self.service.process_peg_data(input_data, cell_ids, time_range)
                
                end_time = time.perf_counter()
                execution_time = (end_time - start_time) * 1000  # ms
                
                times.append(execution_time)
                
                # ê²°ê³¼ ê²€ì¦
                assert response.total_cells_analyzed == test_case['cells']
                assert response.total_pegs_analyzed == test_case['cells'] * 3  # ì…€ë‹¹ 3ê°œ PEG
            
            # í†µê³„ ê³„ì‚°
            avg_time = statistics.mean(times)
            min_time = min(times)
            max_time = max(times)
            std_dev = statistics.stdev(times) if len(times) > 1 else 0
            
            target_time = self.performance_targets[test_case['name']]
            meets_target = avg_time < target_time
            
            results[test_case['name']] = {
                "cells": test_case['cells'],
                "iterations": test_case['iterations'],
                "avg_time_ms": round(avg_time, 2),
                "min_time_ms": round(min_time, 2),
                "max_time_ms": round(max_time, 2),
                "std_dev_ms": round(std_dev, 2),
                "target_ms": target_time,
                "meets_target": meets_target,
                "performance_ratio": round(avg_time / target_time, 3)
            }
            
            status = "âœ…" if meets_target else "âŒ"
            print(f"    {status} í‰ê· : {avg_time:.2f}ms (ëª©í‘œ: {target_time}ms, "
                  f"ë¹„ìœ¨: {avg_time/target_time:.1%})")
        
        return results
    
    def _run_scalability_tests(self) -> Dict[str, Any]:
        """í™•ì¥ì„± í…ŒìŠ¤íŠ¸"""
        cell_counts = [1, 2, 5, 10, 20, 50]
        results = {}
        
        for cell_count in cell_counts:
            print(f"  ğŸ“ˆ {cell_count}ì…€ í™•ì¥ì„± í…ŒìŠ¤íŠ¸")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            cell_ids = [f"scale_cell_{i:03d}" for i in range(cell_count)]
            input_data = {"ems_ip": "192.168.200.2"}
            time_range = {"start": datetime.now()}
            
            # 3íšŒ ì¸¡ì • í›„ í‰ê· 
            times = []
            for _ in range(3):
                start_time = time.perf_counter()
                response = self.service.process_peg_data(input_data, cell_ids, time_range)
                end_time = time.perf_counter()
                
                execution_time = (end_time - start_time) * 1000
                times.append(execution_time)
            
            avg_time = statistics.mean(times)
            
            # ì…€ë‹¹ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            time_per_cell = avg_time / cell_count
            
            results[f"{cell_count}_cells"] = {
                "cell_count": cell_count,
                "total_time_ms": round(avg_time, 2),
                "time_per_cell_ms": round(time_per_cell, 2),
                "pegs_analyzed": response.total_pegs_analyzed,
                "linear_scaling": cell_count <= 10 or time_per_cell < 100  # 100ms/ì…€ ì´í•˜ë©´ ì–‘í˜¸
            }
            
            print(f"    â±ï¸ {avg_time:.2f}ms (ì…€ë‹¹ {time_per_cell:.2f}ms)")
        
        # ì„ í˜• í™•ì¥ì„± ë¶„ì„
        scaling_analysis = self._analyze_scaling_linearity(results)
        results["scaling_analysis"] = scaling_analysis
        
        return results
    
    def _run_memory_tests(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        print("  ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„")
        
        # ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
        tracemalloc.start()
        
        # ê¸°ë³¸ ë©”ëª¨ë¦¬ ìƒíƒœ
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¡œ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
        cell_ids = [f"memory_cell_{i:03d}" for i in range(20)]
        input_data = {"ems_ip": "192.168.200.3"}
        time_range = {"start": datetime.now()}
        
        # ë©”ëª¨ë¦¬ ì¸¡ì •
        start_memory = process.memory_info().rss / 1024 / 1024
        
        response = self.service.process_peg_data(input_data, cell_ids, time_range)
        
        end_memory = process.memory_info().rss / 1024 / 1024
        memory_increase = end_memory - start_memory
        
        # tracemalloc í†µê³„
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³„ì‚° (MB per cell)
        memory_per_cell = memory_increase / len(cell_ids)
        
        results = {
            "initial_memory_mb": round(initial_memory, 2),
            "start_memory_mb": round(start_memory, 2),
            "end_memory_mb": round(end_memory, 2),
            "memory_increase_mb": round(memory_increase, 2),
            "memory_per_cell_mb": round(memory_per_cell, 3),
            "tracemalloc_current_mb": round(current / 1024 / 1024, 2),
            "tracemalloc_peak_mb": round(peak / 1024 / 1024, 2),
            "cells_tested": len(cell_ids),
            "memory_efficient": memory_per_cell < 1.0  # 1MB/ì…€ ì´í•˜ë©´ íš¨ìœ¨ì 
        }
        
        print(f"    ğŸ“Š ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase:.2f}MB "
              f"(ì…€ë‹¹ {memory_per_cell:.3f}MB)")
        print(f"    ğŸ” Peak ë©”ëª¨ë¦¬: {peak/1024/1024:.2f}MB")
        
        return results
    
    def _run_phase_performance_analysis(self) -> Dict[str, Any]:
        """ë‹¨ê³„ë³„ ì„±ëŠ¥ ë¶„ì„"""
        print("  ğŸ“Š ë‹¨ê³„ë³„ ì„±ëŠ¥ ë¶„ì„")
        
        # í‘œì¤€ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        cell_ids = [f"phase_cell_{i:03d}" for i in range(10)]
        input_data = {"ems_ip": "192.168.200.4"}
        time_range = {"start": datetime.now()}
        
        # ë‹¨ê³„ë³„ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì‹¤í–‰
        phase_times = {}
        
        # Mock ë°ì´í„° ìƒì„±
        peg_data = self._generate_test_peg_data(cell_ids)
        
        # 1ë‹¨ê³„: ë°ì´í„° ì¡°íšŒ (Mockì´ë¯€ë¡œ ì œì™¸)
        
        # 2ë‹¨ê³„: ë°ì´í„° ê²€ì¦
        start_time = time.perf_counter()
        validated_data = peg_data  # ì´ë¯¸ ê²€ì¦ëœ ë°ì´í„°
        validation_time = (time.perf_counter() - start_time) * 1000
        
        # 3ë‹¨ê³„: í•„í„°ë§
        start_time = time.perf_counter()
        filtering_result = self.service._run_filtering(validated_data)
        filtering_time = (time.perf_counter() - start_time) * 1000
        
        # 4ë‹¨ê³„: ì§‘ê³„
        start_time = time.perf_counter()
        aggregated_data = self.service._aggregation(validated_data, filtering_result)
        aggregation_time = (time.perf_counter() - start_time) * 1000
        
        # 5ë‹¨ê³„: íŒŒìƒ ê³„ì‚°
        start_time = time.perf_counter()
        derived_data = self.service._derived_calculation(aggregated_data)
        derivation_time = (time.perf_counter() - start_time) * 1000
        
        # 6ë‹¨ê³„: íŒì •
        start_time = time.perf_counter()
        judgement_result = self.service._run_judgement(derived_data, filtering_result)
        judgement_time = (time.perf_counter() - start_time) * 1000
        
        # 7ë‹¨ê³„: í¬ë§·íŒ…
        start_time = time.perf_counter()
        processing_results = {
            "filtering": filtering_result,
            "aggregation": aggregated_data,
            "derived_calculation": derived_data,
            "judgement": judgement_result
        }
        response = self.service._result_formatting(processing_results, [])
        formatting_time = (time.perf_counter() - start_time) * 1000
        
        total_time = validation_time + filtering_time + aggregation_time + derivation_time + judgement_time + formatting_time
        
        phase_times = {
            "validation": round(validation_time, 3),
            "filtering": round(filtering_time, 3),
            "aggregation": round(aggregation_time, 3),
            "derivation": round(derivation_time, 3),
            "judgement": round(judgement_time, 3),
            "formatting": round(formatting_time, 3),
            "total": round(total_time, 3)
        }
        
        # ê° ë‹¨ê³„ë³„ ë¹„ìœ¨ ê³„ì‚°
        phase_percentages = {
            phase: round((time_ms / total_time) * 100, 1) if total_time > 0 else 0
            for phase, time_ms in phase_times.items() if phase != "total"
        }
        
        results = {
            "cells_analyzed": len(cell_ids),
            "phase_times_ms": phase_times,
            "phase_percentages": phase_percentages,
            "bottleneck_phase": max(phase_percentages.items(), key=lambda x: x[1])[0],
            "meets_target": total_time < self.performance_targets["standard_workload"]
        }
        
        print(f"    â±ï¸ ì´ ì‹œê°„: {total_time:.3f}ms")
        print(f"    ğŸ”¥ ë³‘ëª©: {results['bottleneck_phase']} ({phase_percentages[results['bottleneck_phase']]}%)")
        
        return results
    
    def _run_scalability_tests(self) -> Dict[str, Any]:
        """í™•ì¥ì„± í…ŒìŠ¤íŠ¸"""
        cell_counts = [1, 2, 5, 10, 20, 50]
        scalability_data = []
        
        for cell_count in cell_counts:
            print(f"  ğŸ“ˆ {cell_count}ì…€ í™•ì¥ì„± ì¸¡ì •")
            
            # 3íšŒ ì¸¡ì •
            times = []
            for _ in range(3):
                cell_ids = [f"scale_cell_{i:03d}" for i in range(cell_count)]
                input_data = {"ems_ip": "192.168.200.5"}
                time_range = {"start": datetime.now()}
                
                start_time = time.perf_counter()
                response = self.service.process_peg_data(input_data, cell_ids, time_range)
                end_time = time.perf_counter()
                
                execution_time = (end_time - start_time) * 1000
                times.append(execution_time)
            
            avg_time = statistics.mean(times)
            time_per_cell = avg_time / cell_count
            
            scalability_data.append({
                "cell_count": cell_count,
                "avg_time_ms": round(avg_time, 2),
                "time_per_cell_ms": round(time_per_cell, 2),
                "pegs_analyzed": response.total_pegs_analyzed
            })
            
            print(f"    â±ï¸ {avg_time:.2f}ms (ì…€ë‹¹ {time_per_cell:.2f}ms)")
        
        # ì„ í˜•ì„± ë¶„ì„
        linearity_score = self._calculate_linearity_score(scalability_data)
        
        return {
            "scalability_data": scalability_data,
            "linearity_score": linearity_score,
            "is_linear": linearity_score > 0.8,
            "max_time_per_cell": max(data["time_per_cell_ms"] for data in scalability_data)
        }
    
    def _run_memory_tests(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        memory_tests = [
            {"name": "small_dataset", "cells": 5, "samples_per_peg": 20},
            {"name": "medium_dataset", "cells": 10, "samples_per_peg": 40},
            {"name": "large_dataset", "cells": 20, "samples_per_peg": 100}
        ]
        
        results = {}
        
        for test in memory_tests:
            print(f"  ğŸ§  {test['name']}: {test['cells']}ì…€ Ã— {test['samples_per_peg']}ìƒ˜í”Œ")
            
            # ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
            tracemalloc.start()
            process = psutil.Process()
            
            start_memory = process.memory_info().rss / 1024 / 1024
            
            # í° ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            large_peg_data = self._generate_large_test_data(test['cells'], test['samples_per_peg'])
            
            # ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
            filtering_result = self.service._run_filtering(large_peg_data)
            aggregated_data = self.service._aggregation(large_peg_data, filtering_result)
            derived_data = self.service._derived_calculation(aggregated_data)
            judgement_result = self.service._run_judgement(derived_data, filtering_result)
            
            end_memory = process.memory_info().rss / 1024 / 1024
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            memory_increase = end_memory - start_memory
            total_samples = test['cells'] * 3 * test['samples_per_peg'] * 2  # ì…€Ã—PEGÃ—ìƒ˜í”ŒÃ—pre/post
            
            results[test['name']] = {
                "cells": test['cells'],
                "samples_per_peg": test['samples_per_peg'],
                "total_samples": total_samples,
                "memory_increase_mb": round(memory_increase, 3),
                "peak_memory_mb": round(peak / 1024 / 1024, 3),
                "memory_per_sample_bytes": round((peak / total_samples), 2) if total_samples > 0 else 0,
                "memory_efficient": memory_increase < 50  # 50MB ì´í•˜ë©´ íš¨ìœ¨ì 
            }
            
            print(f"    ğŸ“Š ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase:.3f}MB, "
                  f"ìƒ˜í”Œë‹¹: {results[test['name']]['memory_per_sample_bytes']:.2f}B")
        
        return results
    
    def _establish_performance_baseline(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ íšŒê·€ ê¸°ì¤€ì„  ì„¤ì •"""
        print("  ğŸ“ ì„±ëŠ¥ ê¸°ì¤€ì„  ì„¤ì • (10ì…€ í‘œì¤€)")
        
        # í‘œì¤€ ì›Œí¬ë¡œë“œë¡œ 10íšŒ ì¸¡ì •
        times = []
        cell_ids = [f"baseline_cell_{i:03d}" for i in range(10)]
        
        for i in range(10):
            input_data = {"ems_ip": f"192.168.200.{10+i}"}
            time_range = {"start": datetime.now()}
            
            start_time = time.perf_counter()
            response = self.service.process_peg_data(input_data, cell_ids, time_range)
            end_time = time.perf_counter()
            
            execution_time = (end_time - start_time) * 1000
            times.append(execution_time)
        
        # í†µê³„ ê³„ì‚°
        baseline = {
            "measurements": len(times),
            "mean_ms": round(statistics.mean(times), 3),
            "median_ms": round(statistics.median(times), 3),
            "std_dev_ms": round(statistics.stdev(times), 3),
            "min_ms": round(min(times), 3),
            "max_ms": round(max(times), 3),
            "p95_ms": round(sorted(times)[int(len(times) * 0.95)], 3),
            "p99_ms": round(sorted(times)[int(len(times) * 0.99)], 3),
            "coefficient_of_variation": round(statistics.stdev(times) / statistics.mean(times), 3),
            "stable_performance": statistics.stdev(times) / statistics.mean(times) < 0.1
        }
        
        print(f"    ğŸ“Š ê¸°ì¤€ì„ : í‰ê·  {baseline['mean_ms']:.3f}ms Â± {baseline['std_dev_ms']:.3f}ms")
        print(f"    ğŸ“ˆ P95: {baseline['p95_ms']:.3f}ms, P99: {baseline['p99_ms']:.3f}ms")
        
        return baseline
    
    def _generate_test_peg_data(self, cell_ids: List[str]) -> Dict[str, List[PegSampleSeries]]:
        """í…ŒìŠ¤íŠ¸ìš© PEG ë°ì´í„° ìƒì„±"""
        peg_data = {}
        
        for cell_id in cell_ids:
            peg_series = []
            
            # ê° ì…€ë‹¹ 3ê°œ PEG
            peg_names = ["AirMacDLThruAvg", "AirMacULThruAvg", "ConnNoAvg"]
            
            for peg_name in peg_names:
                # 20ê°œ ìƒ˜í”Œ ìƒì„±
                pre_samples = [1000.0 + i * 10 for i in range(20)]
                post_samples = [1100.0 + i * 12 for i in range(20)]
                
                series = PegSampleSeries(
                    peg_name=peg_name,
                    cell_id=cell_id,
                    pre_samples=pre_samples,
                    post_samples=post_samples,
                    unit="Kbps" if "Thru" in peg_name else "count"
                )
                
                peg_series.append(series)
            
            peg_data[cell_id] = peg_series
        
        return peg_data
    
    def _generate_large_test_data(self, cell_count: int, samples_per_peg: int) -> Dict[str, List[PegSampleSeries]]:
        """ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        peg_data = {}
        
        for i in range(cell_count):
            cell_id = f"large_cell_{i:03d}"
            peg_series = []
            
            peg_names = ["AirMacDLThruAvg", "AirMacULThruAvg", "ConnNoAvg"]
            
            for peg_name in peg_names:
                # ì§€ì •ëœ ìˆ˜ì˜ ìƒ˜í”Œ ìƒì„±
                pre_samples = [1000.0 + j * 5 for j in range(samples_per_peg)]
                post_samples = [1050.0 + j * 6 for j in range(samples_per_peg)]
                
                series = PegSampleSeries(
                    peg_name=peg_name,
                    cell_id=cell_id,
                    pre_samples=pre_samples,
                    post_samples=post_samples,
                    unit="Kbps" if "Thru" in peg_name else "count"
                )
                
                peg_series.append(series)
            
            peg_data[cell_id] = peg_series
        
        return peg_data
    
    def _analyze_scaling_linearity(self, scalability_results: Dict[str, Any]) -> Dict[str, Any]:
        """í™•ì¥ì„± ì„ í˜•ì„± ë¶„ì„"""
        # ì…€ ìˆ˜ì™€ ì²˜ë¦¬ ì‹œê°„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„
        data_points = []
        for key, result in scalability_results.items():
            if key != "scaling_analysis" and isinstance(result, dict):
                data_points.append((result["cell_count"], result["total_time_ms"]))
        
        if len(data_points) < 2:
            return {"linearity_score": 0.0, "analysis": "Insufficient data"}
        
        # ì„ í˜• íšŒê·€ ê³„ì‚° (ê°„ë‹¨í•œ ìƒê´€ê´€ê³„)
        x_values = [point[0] for point in data_points]
        y_values = [point[1] for point in data_points]
        
        # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        if len(x_values) > 1:
            correlation = self._calculate_correlation(x_values, y_values)
        else:
            correlation = 0.0
        
        return {
            "correlation_coefficient": round(correlation, 3),
            "linearity_score": round(abs(correlation), 3),
            "is_linear": abs(correlation) > 0.8,
            "data_points": data_points
        }
    
    def _calculate_correlation(self, x: List[float], y: List[float]) -> float:
        """í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°"""
        if len(x) != len(y) or len(x) < 2:
            return 0.0
        
        n = len(x)
        mean_x = sum(x) / n
        mean_y = sum(y) / n
        
        numerator = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n))
        sum_sq_x = sum((x[i] - mean_x) ** 2 for i in range(n))
        sum_sq_y = sum((y[i] - mean_y) ** 2 for i in range(n))
        
        denominator = (sum_sq_x * sum_sq_y) ** 0.5
        
        if denominator == 0:
            return 0.0
        
        return numerator / denominator
    
    def _calculate_linearity_score(self, scalability_data: List[Dict[str, Any]]) -> float:
        """ì„ í˜•ì„± ì ìˆ˜ ê³„ì‚°"""
        if len(scalability_data) < 3:
            return 0.0
        
        # ì…€ë‹¹ ì²˜ë¦¬ ì‹œê°„ì˜ ì¼ê´€ì„± í™•ì¸
        times_per_cell = [data["time_per_cell_ms"] for data in scalability_data]
        
        if not times_per_cell:
            return 0.0
        
        # ë³€ë™ê³„ìˆ˜ ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ì„ í˜•ì„± ë†’ìŒ)
        mean_time = statistics.mean(times_per_cell)
        std_dev = statistics.stdev(times_per_cell) if len(times_per_cell) > 1 else 0
        
        coefficient_of_variation = std_dev / mean_time if mean_time > 0 else 1.0
        
        # ì„ í˜•ì„± ì ìˆ˜ (ë³€ë™ê³„ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        linearity_score = max(0.0, 1.0 - coefficient_of_variation)
        
        return linearity_score
    
    def _verify_performance_targets(self, basic_results: Dict[str, Any]) -> Dict[str, bool]:
        """ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ ê²€ì¦"""
        targets_met = {}
        
        for workload_name, result in basic_results.items():
            target_time = self.performance_targets.get(workload_name, float('inf'))
            actual_time = result["avg_time_ms"]
            targets_met[workload_name] = actual_time < target_time
        
        return targets_met
    
    def _save_benchmark_results(self, results: Dict[str, Any]) -> None:
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
        output_file = Path(__file__).parent / f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {output_file.name}")
    
    def _report_benchmark_summary(self, results: Dict[str, Any]) -> None:
        """ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ë³´ê³ """
        print("\n" + "=" * 60)
        print("ğŸ“Š Choi ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìš”ì•½")
        print("=" * 60)
        
        # ê¸°ë³¸ ì„±ëŠ¥ ìš”ì•½
        basic = results["basic_performance"]
        print("ğŸš€ ê¸°ë³¸ ì„±ëŠ¥:")
        for workload, data in basic.items():
            status = "âœ…" if data["meets_target"] else "âŒ"
            print(f"  {status} {workload}: {data['avg_time_ms']:.2f}ms "
                  f"(ëª©í‘œ: {data['target_ms']}ms, ë¹„ìœ¨: {data['performance_ratio']:.1%})")
        
        # í™•ì¥ì„± ìš”ì•½
        scalability = results["scalability"]
        print(f"\nğŸ“ˆ í™•ì¥ì„±:")
        print(f"  ì„ í˜•ì„± ì ìˆ˜: {scalability['linearity_score']:.3f}")
        print(f"  ì„ í˜• í™•ì¥: {'âœ…' if scalability['is_linear'] else 'âŒ'}")
        print(f"  ìµœëŒ€ ì…€ë‹¹ ì‹œê°„: {scalability['max_time_per_cell']:.2f}ms")
        
        # ë©”ëª¨ë¦¬ ìš”ì•½
        memory = results["memory_usage"]
        print(f"\nğŸ§  ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±:")
        for test_name, data in memory.items():
            if isinstance(data, dict):
                efficient = "âœ…" if data["memory_efficient"] else "âŒ"
                print(f"  {efficient} {test_name}: {data['memory_increase_mb']:.3f}MB "
                      f"(ìƒ˜í”Œë‹¹ {data['memory_per_sample_bytes']:.2f}B)")
        
        # ë‹¨ê³„ë³„ ì„±ëŠ¥
        phases = results["phase_analysis"]
        print(f"\nâš¡ ë‹¨ê³„ë³„ ì„±ëŠ¥:")
        print(f"  ì´ ì²˜ë¦¬ ì‹œê°„: {phases['phase_times_ms']['total']:.3f}ms")
        print(f"  ì£¼ìš” ë³‘ëª©: {phases['bottleneck_phase']} ({phases['phase_percentages'][phases['bottleneck_phase']]}%)")
        
        # ì „ì²´ í‰ê°€
        all_targets_met = all(results["targets_met"].values())
        overall_status = "ğŸ‰ ìš°ìˆ˜" if all_targets_met else "âš ï¸ ê°œì„  í•„ìš”"
        print(f"\nğŸ† ì „ì²´ í‰ê°€: {overall_status}")
        
        print("=" * 60)


# =============================================================================
# ì§ì ‘ ì‹¤í–‰
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        benchmark = ChoiPerformanceBenchmark()
        results = benchmark.run_comprehensive_benchmark()
        
        # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ì— ë”°ë¥¸ ì¢…ë£Œ ì½”ë“œ
        all_targets_met = all(results["targets_met"].values())
        
        if all_targets_met:
            print("\nğŸ‰ ëª¨ë“  ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±!")
            sys.exit(0)
        else:
            print("\nâš ï¸ ì¼ë¶€ ì„±ëŠ¥ ëª©í‘œ ë¯¸ë‹¬ì„±")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
