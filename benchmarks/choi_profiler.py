"""
Choi ì•Œê³ ë¦¬ì¦˜ ìƒì„¸ í”„ë¡œíŒŒì¼ë§ ë„êµ¬

ì´ ëª¨ë“ˆì€ cProfileê³¼ ë‹¤ì–‘í•œ í”„ë¡œíŒŒì¼ë§ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬
Choi ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ ë³‘ëª© ì§€ì ì„ ì •ë°€ ë¶„ì„í•©ë‹ˆë‹¤.

Author: Choi Algorithm Profiling Team
Created: 2025-09-20
"""

import cProfile
import pstats
import io
import time
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Tuple
import sys
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.services.peg_processing_service import PEGProcessingService
from app.models.judgement import PegSampleSeries

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.ERROR)  # í”„ë¡œíŒŒì¼ë§ ì‹œ ë¡œê·¸ ìµœì†Œí™”
logger = logging.getLogger(__name__)


class ChoiAlgorithmProfiler:
    """
    Choi ì•Œê³ ë¦¬ì¦˜ í”„ë¡œíŒŒì¼ëŸ¬
    
    cProfileì„ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ë³„ ì‹¤í–‰ ì‹œê°„ê³¼ í˜¸ì¶œ ë¹ˆë„ë¥¼ ë¶„ì„í•˜ê³ 
    ì„±ëŠ¥ ë³‘ëª© ì§€ì ì„ ì‹ë³„í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """í”„ë¡œíŒŒì¼ëŸ¬ ì´ˆê¸°í™”"""
        self.service = PEGProcessingService()
        self.profiling_results = {}
        
        logger.info("Choi algorithm profiler initialized")
    
    def run_comprehensive_profiling(self) -> Dict[str, Any]:
        """í¬ê´„ì  í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰"""
        print("ğŸ” Choi ì•Œê³ ë¦¬ì¦˜ ìƒì„¸ í”„ë¡œíŒŒì¼ë§")
        print("=" * 60)
        
        try:
            # 1. ì „ì²´ ì›Œí¬í”Œë¡œìš° í”„ë¡œíŒŒì¼ë§
            print("1. ì „ì²´ ì›Œí¬í”Œë¡œìš° í”„ë¡œíŒŒì¼ë§:")
            full_workflow_profile = self._profile_full_workflow()
            
            # 2. ë‹¨ê³„ë³„ í”„ë¡œíŒŒì¼ë§
            print("\n2. ë‹¨ê³„ë³„ í”„ë¡œíŒŒì¼ë§:")
            phase_profiles = self._profile_individual_phases()
            
            # 3. í•«ìŠ¤íŒŸ ë¶„ì„
            print("\n3. í•«ìŠ¤íŒŸ ë¶„ì„:")
            hotspot_analysis = self._analyze_hotspots(full_workflow_profile)
            
            # 4. í•¨ìˆ˜ í˜¸ì¶œ ë¹ˆë„ ë¶„ì„
            print("\n4. í•¨ìˆ˜ í˜¸ì¶œ ë¹ˆë„ ë¶„ì„:")
            call_frequency_analysis = self._analyze_call_frequencies(full_workflow_profile)
            
            # 5. ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
            print("\n5. ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§:")
            memory_profile = self._profile_memory_usage()
            
            # ì¢…í•© ê²°ê³¼
            comprehensive_results = {
                "timestamp": datetime.now().isoformat(),
                "full_workflow": full_workflow_profile,
                "phase_breakdown": phase_profiles,
                "hotspots": hotspot_analysis,
                "call_frequencies": call_frequency_analysis,
                "memory_profile": memory_profile,
                "optimization_recommendations": self._generate_optimization_recommendations(
                    hotspot_analysis, call_frequency_analysis
                )
            }
            
            # ê²°ê³¼ ì €ì¥
            self._save_profiling_results(comprehensive_results)
            
            # ìš”ì•½ ë³´ê³ 
            self._report_profiling_summary(comprehensive_results)
            
            return comprehensive_results
            
        except Exception as e:
            logger.error(f"Profiling execution failed: {e}")
            raise
    
    def _profile_full_workflow(self) -> Dict[str, Any]:
        """ì „ì²´ ì›Œí¬í”Œë¡œìš° í”„ë¡œíŒŒì¼ë§"""
        print("  ğŸ”„ ì „ì²´ ì›Œí¬í”Œë¡œìš° cProfile ì‹¤í–‰")
        
        # í‘œì¤€ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        cell_ids = [f"profile_cell_{i:03d}" for i in range(10)]
        input_data = {"ems_ip": "192.168.200.10"}
        time_range = {"start": datetime.now()}
        
        # cProfile ì‹¤í–‰
        profiler = cProfile.Profile()
        
        profiler.enable()
        
        # ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
        response = self.service.process_peg_data(input_data, cell_ids, time_range)
        
        profiler.disable()
        
        # í”„ë¡œíŒŒì¼ ê²°ê³¼ ë¶„ì„
        stats_stream = io.StringIO()
        stats = pstats.Stats(profiler, stream=stats_stream)
        
        # ë‹¤ì–‘í•œ ì •ë ¬ë¡œ í†µê³„ ìƒì„±
        profile_data = {}
        
        # 1. ëˆ„ì  ì‹œê°„ ê¸°ì¤€ ìƒìœ„ 20ê°œ í•¨ìˆ˜
        stats.sort_stats('cumulative')
        stats.print_stats(20)
        profile_data["by_cumulative_time"] = stats_stream.getvalue()
        
        # 2. ìì²´ ì‹¤í–‰ ì‹œê°„ ê¸°ì¤€ ìƒìœ„ 20ê°œ í•¨ìˆ˜
        stats_stream = io.StringIO()
        stats = pstats.Stats(profiler, stream=stats_stream)
        stats.sort_stats('tottime')
        stats.print_stats(20)
        profile_data["by_self_time"] = stats_stream.getvalue()
        
        # 3. í˜¸ì¶œ íšŸìˆ˜ ê¸°ì¤€ ìƒìœ„ 20ê°œ í•¨ìˆ˜
        stats_stream = io.StringIO()
        stats = pstats.Stats(profiler, stream=stats_stream)
        stats.sort_stats('ncalls')
        stats.print_stats(20)
        profile_data["by_call_count"] = stats_stream.getvalue()
        
        # í”„ë¡œíŒŒì¼ í†µê³„ ì¶”ì¶œ
        stats_dict = {}
        for func_key, (cc, nc, tt, ct, callers) in stats.stats.items():
            filename, line_num, func_name = func_key
            
            # Choi ì•Œê³ ë¦¬ì¦˜ ê´€ë ¨ í•¨ìˆ˜ë§Œ í•„í„°ë§
            if any(keyword in filename for keyword in ['choi', 'anomaly', 'kpi', 'filtering', 'judgement']):
                stats_dict[f"{filename}:{func_name}"] = {
                    "call_count": cc,
                    "total_time": round(tt, 6),
                    "cumulative_time": round(ct, 6),
                    "time_per_call": round(tt / cc, 6) if cc > 0 else 0,
                    "line_number": line_num
                }
        
        return {
            "total_execution_time_ms": response.processing_time_ms,
            "cells_analyzed": response.total_cells_analyzed,
            "profile_text": profile_data,
            "function_stats": stats_dict,
            "profiling_overhead_estimated": "< 5%"
        }
    
    def _profile_individual_phases(self) -> Dict[str, Any]:
        """ë‹¨ê³„ë³„ ê°œë³„ í”„ë¡œíŒŒì¼ë§"""
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        cell_ids = [f"phase_cell_{i:03d}" for i in range(10)]
        peg_data = self._generate_standard_test_data(cell_ids)
        
        phase_results = {}
        
        # ê° ë‹¨ê³„ë³„ë¡œ ê°œë³„ í”„ë¡œíŒŒì¼ë§
        phases = [
            ("filtering", lambda: self.service._run_filtering(peg_data)),
            ("aggregation", lambda data=peg_data: self._profile_aggregation(data)),
            ("derivation", lambda data=peg_data: self._profile_derivation(data)),
            ("judgement", lambda data=peg_data: self._profile_judgement(data))
        ]
        
        for phase_name, phase_func in phases:
            print(f"  ğŸ” {phase_name} ë‹¨ê³„ í”„ë¡œíŒŒì¼ë§")
            
            # 3íšŒ ì¸¡ì •
            times = []
            for _ in range(3):
                start_time = time.perf_counter()
                result = phase_func()
                end_time = time.perf_counter()
                
                execution_time = (end_time - start_time) * 1000
                times.append(execution_time)
            
            avg_time = sum(times) / len(times)
            
            phase_results[phase_name] = {
                "avg_time_ms": round(avg_time, 3),
                "min_time_ms": round(min(times), 3),
                "max_time_ms": round(max(times), 3),
                "measurements": len(times)
            }
            
            print(f"    â±ï¸ {phase_name}: {avg_time:.3f}ms")
        
        return phase_results
    
    def _profile_aggregation(self, peg_data: Dict[str, List[PegSampleSeries]]) -> Any:
        """ì§‘ê³„ ë‹¨ê³„ í”„ë¡œíŒŒì¼ë§"""
        filtering_result = self.service._run_filtering(peg_data)
        return self.service._aggregation(peg_data, filtering_result)
    
    def _profile_derivation(self, peg_data: Dict[str, List[PegSampleSeries]]) -> Any:
        """íŒŒìƒ ê³„ì‚° ë‹¨ê³„ í”„ë¡œíŒŒì¼ë§"""
        filtering_result = self.service._run_filtering(peg_data)
        aggregated_data = self.service._aggregation(peg_data, filtering_result)
        return self.service._derived_calculation(aggregated_data)
    
    def _profile_judgement(self, peg_data: Dict[str, List[PegSampleSeries]]) -> Any:
        """íŒì • ë‹¨ê³„ í”„ë¡œíŒŒì¼ë§"""
        filtering_result = self.service._run_filtering(peg_data)
        aggregated_data = self.service._aggregation(peg_data, filtering_result)
        derived_data = self.service._derived_calculation(aggregated_data)
        return self.service._run_judgement(derived_data, filtering_result)
    
    def _analyze_hotspots(self, profile_data: Dict[str, Any]) -> Dict[str, Any]:
        """í•«ìŠ¤íŒŸ ë¶„ì„"""
        function_stats = profile_data["function_stats"]
        
        if not function_stats:
            return {"hotspots": [], "analysis": "No function stats available"}
        
        # ëˆ„ì  ì‹œê°„ ê¸°ì¤€ ìƒìœ„ 10ê°œ í•¨ìˆ˜
        hotspots_by_cumtime = sorted(
            function_stats.items(),
            key=lambda x: x[1]["cumulative_time"],
            reverse=True
        )[:10]
        
        # ìì²´ ì‹¤í–‰ ì‹œê°„ ê¸°ì¤€ ìƒìœ„ 10ê°œ í•¨ìˆ˜
        hotspots_by_selftime = sorted(
            function_stats.items(),
            key=lambda x: x[1]["total_time"],
            reverse=True
        )[:10]
        
        # í˜¸ì¶œ íšŸìˆ˜ ê¸°ì¤€ ìƒìœ„ 10ê°œ í•¨ìˆ˜
        hotspots_by_calls = sorted(
            function_stats.items(),
            key=lambda x: x[1]["call_count"],
            reverse=True
        )[:10]
        
        hotspots = {
            "by_cumulative_time": [
                {
                    "function": func_name,
                    "cumulative_time_ms": round(stats["cumulative_time"] * 1000, 3),
                    "call_count": stats["call_count"],
                    "time_per_call_ms": round(stats["time_per_call"] * 1000, 6)
                }
                for func_name, stats in hotspots_by_cumtime
            ],
            "by_self_time": [
                {
                    "function": func_name,
                    "self_time_ms": round(stats["total_time"] * 1000, 3),
                    "call_count": stats["call_count"],
                    "time_per_call_ms": round(stats["time_per_call"] * 1000, 6)
                }
                for func_name, stats in hotspots_by_selftime
            ],
            "by_call_frequency": [
                {
                    "function": func_name,
                    "call_count": stats["call_count"],
                    "total_time_ms": round(stats["total_time"] * 1000, 3),
                    "avg_time_per_call_us": round(stats["time_per_call"] * 1000000, 2)
                }
                for func_name, stats in hotspots_by_calls
            ]
        }
        
        # ìƒìœ„ 3ê°œ í•«ìŠ¤íŒŸ ì¶œë ¥
        print("    ğŸ”¥ ìƒìœ„ í•«ìŠ¤íŒŸ (ëˆ„ì  ì‹œê°„):")
        for i, hotspot in enumerate(hotspots["by_cumulative_time"][:3], 1):
            print(f"      {i}. {hotspot['function'].split(':')[-1]}: {hotspot['cumulative_time_ms']:.3f}ms")
        
        return hotspots
    
    def _analyze_call_frequencies(self, profile_data: Dict[str, Any]) -> Dict[str, Any]:
        """í•¨ìˆ˜ í˜¸ì¶œ ë¹ˆë„ ë¶„ì„"""
        function_stats = profile_data["function_stats"]
        
        if not function_stats:
            return {"analysis": "No function stats available"}
        
        # í˜¸ì¶œ ë¹ˆë„ í†µê³„
        call_counts = [stats["call_count"] for stats in function_stats.values()]
        
        if not call_counts:
            return {"analysis": "No call count data"}
        
        total_calls = sum(call_counts)
        avg_calls = total_calls / len(call_counts)
        max_calls = max(call_counts)
        
        # ìì£¼ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜ë“¤ (ìƒìœ„ 20%)
        high_frequency_threshold = sorted(call_counts, reverse=True)[int(len(call_counts) * 0.2)]
        
        high_frequency_functions = [
            {
                "function": func_name.split(':')[-1],
                "call_count": stats["call_count"],
                "total_time_ms": round(stats["total_time"] * 1000, 3),
                "avg_time_per_call_us": round(stats["time_per_call"] * 1000000, 2)
            }
            for func_name, stats in function_stats.items()
            if stats["call_count"] >= high_frequency_threshold
        ]
        
        # í˜¸ì¶œ ë¹ˆë„ë³„ ì •ë ¬
        high_frequency_functions.sort(key=lambda x: x["call_count"], reverse=True)
        
        analysis = {
            "total_function_calls": total_calls,
            "unique_functions": len(function_stats),
            "avg_calls_per_function": round(avg_calls, 1),
            "max_calls_single_function": max_calls,
            "high_frequency_functions": high_frequency_functions[:10],
            "call_distribution": {
                "functions_called_once": len([c for c in call_counts if c == 1]),
                "functions_called_2_10": len([c for c in call_counts if 2 <= c <= 10]),
                "functions_called_10_plus": len([c for c in call_counts if c > 10])
            }
        }
        
        print(f"    ğŸ“ ì´ í•¨ìˆ˜ í˜¸ì¶œ: {total_calls:,}íšŒ")
        print(f"    ğŸ”„ ìµœë‹¤ í˜¸ì¶œ í•¨ìˆ˜: {max_calls:,}íšŒ")
        
        return analysis
    
    def _profile_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§"""
        try:
            import tracemalloc
            import psutil
            
            # ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
            tracemalloc.start()
            process = psutil.Process()
            
            initial_memory = process.memory_info().rss / 1024 / 1024
            
            # ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¡œ ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
            cell_ids = [f"memory_profile_cell_{i:03d}" for i in range(20)]
            input_data = {"ems_ip": "192.168.200.20"}
            time_range = {"start": datetime.now()}
            
            # ë‹¨ê³„ë³„ ë©”ëª¨ë¦¬ ì¸¡ì •
            memory_snapshots = []
            
            # ì´ˆê¸° ìƒíƒœ
            memory_snapshots.append({
                "phase": "initial",
                "memory_mb": process.memory_info().rss / 1024 / 1024
            })
            
            # ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
            response = self.service.process_peg_data(input_data, cell_ids, time_range)
            
            # ìµœì¢… ìƒíƒœ
            final_memory = process.memory_info().rss / 1024 / 1024
            memory_snapshots.append({
                "phase": "final",
                "memory_mb": final_memory
            })
            
            # tracemalloc í†µê³„
            current, peak = tracemalloc.get_traced_memory()
            
            # ë©”ëª¨ë¦¬ í†µê³„ ìƒì„±
            memory_stats = tracemalloc.take_snapshot().statistics('lineno')
            
            # ìƒìœ„ ë©”ëª¨ë¦¬ ì‚¬ìš© í•¨ìˆ˜ë“¤
            top_memory_users = []
            for stat in memory_stats[:10]:
                try:
                    traceback_info = stat.traceback.format()[0] if stat.traceback.format() else "unknown"
                    if any(keyword in traceback_info for keyword in ['choi', 'anomaly', 'kpi']):
                        top_memory_users.append({
                            "file": traceback_info.split('/')[-1] if '/' in traceback_info else traceback_info,
                            "size_mb": round(stat.size / 1024 / 1024, 3),
                            "count": stat.count
                        })
                except Exception:
                    # tracemalloc í†µê³„ ì ‘ê·¼ ì˜¤ë¥˜ ì‹œ ê±´ë„ˆë›°ê¸°
                    continue
            
            tracemalloc.stop()
            
            results = {
                "initial_memory_mb": round(initial_memory, 3),
                "final_memory_mb": round(final_memory, 3),
                "memory_increase_mb": round(final_memory - initial_memory, 3),
                "peak_traced_memory_mb": round(peak / 1024 / 1024, 3),
                "current_traced_memory_mb": round(current / 1024 / 1024, 3),
                "cells_processed": len(cell_ids),
                "memory_per_cell_mb": round((final_memory - initial_memory) / len(cell_ids), 4),
                "top_memory_users": top_memory_users,
                "memory_snapshots": memory_snapshots
            }
            
            print(f"    ğŸ§  ë©”ëª¨ë¦¬ ì¦ê°€: {results['memory_increase_mb']:.3f}MB")
            print(f"    ğŸ“Š Peak ë©”ëª¨ë¦¬: {results['peak_traced_memory_mb']:.3f}MB")
            
            return results
            
        except ImportError:
            return {"error": "tracemalloc or psutil not available"}
    
    def _generate_standard_test_data(self, cell_ids: List[str]) -> Dict[str, List[PegSampleSeries]]:
        """í‘œì¤€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        peg_data = {}
        
        for cell_id in cell_ids:
            peg_series = []
            peg_names = ["AirMacDLThruAvg", "AirMacULThruAvg", "ConnNoAvg"]
            
            for peg_name in peg_names:
                # í‘œì¤€ 20ê°œ ìƒ˜í”Œ
                pre_samples = [1000.0 + i * 10 for i in range(20)]
                post_samples = [1100.0 + i * 12 for i in range(20)]
                
                series = PegSampleSeries(
                    peg_name=peg_name,
                    cell_id=cell_id,
                    pre_samples=pre_samples,
                    post_samples=post_samples,
                    unit="Kbps" if "Thru" in peg_name else "count"
                )
                
                peg_series.append(series)
            
            peg_data[cell_id] = peg_series
        
        return peg_data
    
    def _generate_optimization_recommendations(self, 
                                             hotspots: Dict[str, Any], 
                                             call_frequencies: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # 1. ëˆ„ì  ì‹œê°„ ê¸°ì¤€ ìµœì í™”
        if hotspots.get("by_cumulative_time"):
            top_cumulative = hotspots["by_cumulative_time"][0]
            if top_cumulative["cumulative_time_ms"] > 1.0:  # 1ms ì´ìƒ
                recommendations.append({
                    "type": "cumulative_time_optimization",
                    "priority": "high",
                    "function": top_cumulative["function"],
                    "current_time_ms": top_cumulative["cumulative_time_ms"],
                    "recommendation": "ì´ í•¨ìˆ˜ì˜ ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ë¥¼ ê²€í† í•˜ê³  numpy ë²¡í„°í™” ì ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.",
                    "potential_improvement": "20-50% ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥"
                })
        
        # 2. í˜¸ì¶œ ë¹ˆë„ ê¸°ì¤€ ìµœì í™”
        if call_frequencies.get("high_frequency_functions"):
            top_frequent = call_frequencies["high_frequency_functions"][0]
            if top_frequent["call_count"] > 1000:  # 1000íšŒ ì´ìƒ í˜¸ì¶œ
                recommendations.append({
                    "type": "call_frequency_optimization",
                    "priority": "medium",
                    "function": top_frequent["function"],
                    "call_count": top_frequent["call_count"],
                    "recommendation": "ìì£¼ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜ëŠ” ê²°ê³¼ ìºì‹±ì´ë‚˜ ê³„ì‚° ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.",
                    "potential_improvement": "10-30% ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥"
                })
        
        # 3. ë©”ëª¨ë¦¬ ìµœì í™”
        recommendations.append({
            "type": "memory_optimization",
            "priority": "low",
            "recommendation": "í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ íš¨ìœ¨ì ì´ì§€ë§Œ, ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ numpy ë°°ì—´ ì¬ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.",
            "potential_improvement": "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 10-20% ê°ì†Œ ê°€ëŠ¥"
        })
        
        # 4. ì•Œê³ ë¦¬ì¦˜ ìµœì í™”
        recommendations.append({
            "type": "algorithmic_optimization", 
            "priority": "low",
            "recommendation": "í•„í„°ë§ ë‹¨ê³„ê°€ ì£¼ìš” ë³‘ëª©ì´ë¯€ë¡œ, ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ì´ë‚˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.",
            "potential_improvement": "ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ 30-50% ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥"
        })
        
        return recommendations
    
    def _save_profiling_results(self, results: Dict[str, Any]) -> None:
        """í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ê²°ê³¼ ì €ì¥
        json_file = Path(__file__).parent / f"profiling_report_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            # í…ìŠ¤íŠ¸ í”„ë¡œíŒŒì¼ ë°ì´í„°ëŠ” ì œì™¸í•˜ê³  ì €ì¥
            save_data = {k: v for k, v in results.items() if k != "full_workflow"}
            save_data["full_workflow_summary"] = {
                "total_execution_time_ms": results["full_workflow"]["total_execution_time_ms"],
                "cells_analyzed": results["full_workflow"]["cells_analyzed"],
                "function_count": len(results["full_workflow"]["function_stats"])
            }
            json.dump(save_data, f, indent=2, ensure_ascii=False, default=str)
        
        # í…ìŠ¤íŠ¸ í”„ë¡œíŒŒì¼ ê²°ê³¼ ì €ì¥
        text_file = Path(__file__).parent / f"cprofile_output_{timestamp}.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write("=== Choi Algorithm cProfile Output ===\n\n")
            for key, content in results["full_workflow"]["profile_text"].items():
                f.write(f"=== {key.upper()} ===\n")
                f.write(content)
                f.write("\n\n")
        
        print(f"  ğŸ’¾ í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì €ì¥: {json_file.name}, {text_file.name}")
    
    def _report_profiling_summary(self, results: Dict[str, Any]) -> None:
        """í”„ë¡œíŒŒì¼ë§ ìš”ì•½ ë³´ê³ """
        print("\n" + "=" * 60)
        print("ğŸ” Choi ì•Œê³ ë¦¬ì¦˜ í”„ë¡œíŒŒì¼ë§ ìš”ì•½")
        print("=" * 60)
        
        # ì „ì²´ ì„±ëŠ¥
        full_workflow = results["full_workflow"]
        print(f"â±ï¸ ì „ì²´ ì‹¤í–‰ ì‹œê°„: {full_workflow['total_execution_time_ms']:.3f}ms")
        print(f"ğŸ“Š ë¶„ì„ëœ ì…€: {full_workflow['cells_analyzed']}ê°œ")
        print(f"ğŸ”§ í”„ë¡œíŒŒì¼ëœ í•¨ìˆ˜: {len(full_workflow['function_stats'])}ê°œ")
        
        # ë‹¨ê³„ë³„ ì„±ëŠ¥
        phases = results["phase_breakdown"]
        print(f"\nâš¡ ë‹¨ê³„ë³„ ì„±ëŠ¥:")
        total_phase_time = sum(phase["avg_time_ms"] for phase in phases.values())
        for phase_name, phase_data in phases.items():
            percentage = (phase_data["avg_time_ms"] / total_phase_time) * 100 if total_phase_time > 0 else 0
            print(f"  {phase_name}: {phase_data['avg_time_ms']:.3f}ms ({percentage:.1f}%)")
        
        # ìµœì í™” ê¶Œì¥ì‚¬í•­
        recommendations = results["optimization_recommendations"]
        print(f"\nğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(recommendations, 1):
            priority_icon = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(rec["priority"], "âšª")
            print(f"  {i}. {priority_icon} {rec['type']}: {rec['recommendation']}")
        
        print("=" * 60)


# =============================================================================
# ì§ì ‘ ì‹¤í–‰
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        profiler = ChoiAlgorithmProfiler()
        results = profiler.run_comprehensive_profiling()
        
        print("\nğŸ‰ í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ!")
        print("ğŸ“ ìƒì„¸ ê²°ê³¼ëŠ” ìƒì„±ëœ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ì„¸ìš”.")
        
    except Exception as e:
        print(f"âŒ í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
