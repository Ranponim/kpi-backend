"""
ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ìœ í‹¸ë¦¬í‹°

MongoDB ì»¬ë ‰ì…˜ì— ì ì ˆí•œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì¿¼ë¦¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
"""

import logging
from typing import List, Dict, Any
from motor.motor_asyncio import AsyncIOMotorDatabase
from pymongo import IndexModel, ASCENDING, DESCENDING, TEXT

logger = logging.getLogger(__name__)

async def create_analysis_indexes(db: AsyncIOMotorDatabase):
    """
    ë¶„ì„ ê²°ê³¼ ì»¬ë ‰ì…˜ì— ìµœì í™”ëœ ì¸ë±ìŠ¤ ìƒì„±
    
    ì£¼ìš” ì¿¼ë¦¬ íŒ¨í„´ì— ë§ì¶˜ ë³µí•© ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    collection = db.analysis_results
    
    # ì¸ë±ìŠ¤ ì •ì˜
    indexes = [
        # 1. ê¸°ë³¸ ì¡°íšŒìš© ì¸ë±ìŠ¤ (ë‚ ì§œ ì—­ìˆœ)
        IndexModel([("analysis_date", DESCENDING)], name="idx_analysis_date_desc"),
        
        # 2. NE/Cell ê¸°ë°˜ ì¡°íšŒìš© ë³µí•© ì¸ë±ìŠ¤
        IndexModel([
            ("ne_id", ASCENDING),
            ("cell_id", ASCENDING),
            ("analysis_date", DESCENDING)
        ], name="idx_ne_cell_date"),
        
        # 3. ìƒíƒœë³„ ì¡°íšŒìš© ì¸ë±ìŠ¤
        IndexModel([
            ("status", ASCENDING),
            ("analysis_date", DESCENDING)
        ], name="idx_status_date"),
        
        # 4. ê¸°ê°„ë³„ ê²€ìƒ‰ìš© ì¸ë±ìŠ¤
        IndexModel([
            ("analysis_date", ASCENDING),
            ("status", ASCENDING)
        ], name="idx_date_status"),
        
        # 5. í…ìŠ¤íŠ¸ ê²€ìƒ‰ìš© ì¸ë±ìŠ¤ (NE/Cell ID)
        IndexModel([
            ("ne_id", TEXT),
            ("cell_id", TEXT)
        ], name="idx_text_search"),
        
        # 6. KPI ì„±ëŠ¥ ì¡°íšŒìš© ì¸ë±ìŠ¤
        IndexModel([
            ("results.kpi_name", ASCENDING),
            ("results.status", ASCENDING),
            ("analysis_date", DESCENDING)
        ], name="idx_kpi_performance"),
        
        # 7. í˜ì´ì§€ë„¤ì´ì…˜ ìµœì í™” ì¸ë±ìŠ¤
        IndexModel([("created_at", DESCENDING)], name="idx_created_at_desc"),
        
        # 8. TTL ì¸ë±ìŠ¤ (6ê°œì›” í›„ ìë™ ì‚­ì œ)
        IndexModel([("analysis_date", ASCENDING)], 
                  name="idx_analysis_date_ttl",
                  expireAfterSeconds=15552000)  # 6ê°œì›” = 180ì¼ * 24ì‹œê°„ * 60ë¶„ * 60ì´ˆ
    ]
    
    try:
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
        existing_indexes = await collection.list_indexes().to_list(length=None)
        existing_names = {idx.get('name') for idx in existing_indexes}
        
        # ìƒˆë¡œìš´ ì¸ë±ìŠ¤ë§Œ ìƒì„±
        new_indexes = [idx for idx in indexes if idx.document['name'] not in existing_names]
        
        if new_indexes:
            await collection.create_indexes(new_indexes)
            logger.info(f"ë¶„ì„ ê²°ê³¼ ì»¬ë ‰ì…˜ì— {len(new_indexes)}ê°œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            
            for idx in new_indexes:
                logger.info(f"  - {idx.document['name']}: {idx.document['key']}")
        else:
            logger.info("ë¶„ì„ ê²°ê³¼ ì»¬ë ‰ì…˜: ëª¨ë“  ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬")
            
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

async def create_preference_indexes(db: AsyncIOMotorDatabase):
    """
    ì‚¬ìš©ì ì„¤ì • ì»¬ë ‰ì…˜ì— ì¸ë±ìŠ¤ ìƒì„±
    """
    collection = db.user_preferences
    
    # ì¸ë±ìŠ¤ ìƒì„± ì „ì— ì¤‘ë³µ ë°ì´í„° ì •ë¦¬
    try:
        # user_idê°€ nullì¸ ì¤‘ë³µ ë°ì´í„° í™•ì¸ ë° ì •ë¦¬
        null_user_docs = await collection.find({"user_id": None}).to_list(length=None)
        if len(null_user_docs) > 1:
            logger.warning(f"user_idê°€ nullì¸ ì¤‘ë³µ ë°ì´í„° {len(null_user_docs)}ê°œ ë°œê²¬, ì²« ë²ˆì§¸ë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ")
            # ì²« ë²ˆì§¸ ë¬¸ì„œë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
            docs_to_delete = null_user_docs[1:]
            for doc in docs_to_delete:
                await collection.delete_one({"_id": doc["_id"]})
            logger.info(f"ì¤‘ë³µ ë°ì´í„° {len(docs_to_delete)}ê°œ ì‚­ì œ ì™„ë£Œ")
        
        # user_idê°€ nullì´ ì•„ë‹Œ ì¤‘ë³µ ë°ì´í„°ë„ í™•ì¸
        pipeline = [
            {"$match": {"user_id": {"$ne": None}}},
            {"$group": {"_id": "$user_id", "count": {"$sum": 1}, "docs": {"$push": "$_id"}}},
            {"$match": {"count": {"$gt": 1}}}
        ]
        
        duplicates = await collection.aggregate(pipeline).to_list(length=None)
        for dup in duplicates:
            user_id = dup["_id"]
            doc_ids = dup["docs"]
            logger.warning(f"user_id '{user_id}'ì˜ ì¤‘ë³µ ë°ì´í„° {len(doc_ids)}ê°œ ë°œê²¬, ì²« ë²ˆì§¸ë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ")
            # ì²« ë²ˆì§¸ ë¬¸ì„œë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
            docs_to_delete = doc_ids[1:]
            for doc_id in docs_to_delete:
                await collection.delete_one({"_id": doc_id})
            logger.info(f"user_id '{user_id}'ì˜ ì¤‘ë³µ ë°ì´í„° {len(docs_to_delete)}ê°œ ì‚­ì œ ì™„ë£Œ")
            
    except Exception as e:
        logger.error(f"ì¤‘ë³µ ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì¤‘ë³µ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨í•´ë„ ì¸ë±ìŠ¤ ìƒì„±ì€ ì‹œë„
    
    indexes = [
        # 1. ì‚¬ìš©ì ID ê¸°ë³¸ ì¸ë±ìŠ¤ (ìœ ë‹ˆí¬)
        IndexModel([("user_id", ASCENDING)], unique=True, name="idx_user_id_unique"),
        
        # 2. ìˆ˜ì • ë‚ ì§œ ì¸ë±ìŠ¤
        IndexModel([("updated_at", DESCENDING)], name="idx_updated_at_desc"),
        
        # 3. ì„¤ì • íƒ€ì…ë³„ ì¸ë±ìŠ¤
        IndexModel([
            ("dashboard_settings.theme", ASCENDING),
            ("user_id", ASCENDING)
        ], name="idx_theme_user")
    ]
    
    try:
        existing_indexes = await collection.list_indexes().to_list(length=None)
        existing_names = {idx.get('name') for idx in existing_indexes}
        
        new_indexes = [idx for idx in indexes if idx.document['name'] not in existing_names]
        
        if new_indexes:
            await collection.create_indexes(new_indexes)
            logger.info(f"ì‚¬ìš©ì ì„¤ì • ì»¬ë ‰ì…˜ì— {len(new_indexes)}ê°œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        else:
            logger.info("ì‚¬ìš©ì ì„¤ì • ì»¬ë ‰ì…˜: ëª¨ë“  ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬")
            
    except Exception as e:
        logger.error(f"ì‚¬ìš©ì ì„¤ì • ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

async def create_statistics_indexes(db: AsyncIOMotorDatabase):
    """
    í†µê³„ ë°ì´í„° ì»¬ë ‰ì…˜ì— ì¸ë±ìŠ¤ ìƒì„±
    """
    collection = db.kpi_statistics
    
    indexes = [
        # 1. ë‚ ì§œ ê¸°ê°„ ì¡°íšŒìš© ì¸ë±ìŠ¤
        IndexModel([
            ("date", ASCENDING),
            ("ne_id", ASCENDING)
        ], name="idx_date_ne"),
        
        # 2. KPI íƒ€ì…ë³„ ì¡°íšŒ ì¸ë±ìŠ¤
        IndexModel([
            ("kpi_type", ASCENDING),
            ("date", DESCENDING)
        ], name="idx_kpi_type_date"),
        
        # 3. ë³µí•© ì¡°íšŒ ì¸ë±ìŠ¤
        IndexModel([
            ("ne_id", ASCENDING),
            ("kpi_type", ASCENDING),
            ("date", DESCENDING)
        ], name="idx_ne_kpi_date")
    ]
    
    try:
        existing_indexes = await collection.list_indexes().to_list(length=None)
        existing_names = {idx.get('name') for idx in existing_indexes}
        
        new_indexes = [idx for idx in indexes if idx.document['name'] not in existing_names]
        
        if new_indexes:
            await collection.create_indexes(new_indexes)
            logger.info(f"í†µê³„ ë°ì´í„° ì»¬ë ‰ì…˜ì— {len(new_indexes)}ê°œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        else:
            logger.info("í†µê³„ ë°ì´í„° ì»¬ë ‰ì…˜: ëª¨ë“  ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬")
            
    except Exception as e:
        logger.error(f"í†µê³„ ë°ì´í„° ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

async def create_master_data_indexes(db: AsyncIOMotorDatabase):
    """
    ë§ˆìŠ¤í„° ë°ì´í„° ì»¬ë ‰ì…˜ì— ì¸ë±ìŠ¤ ìƒì„±
    """
    # PEG ë§ˆìŠ¤í„° ë°ì´í„°
    peg_collection = db.peg_master
    peg_indexes = [
        IndexModel([("peg_id", ASCENDING)], unique=True, name="idx_peg_id_unique"),
        IndexModel([("peg_name", TEXT)], name="idx_peg_name_text"),
        IndexModel([("region", ASCENDING)], name="idx_region")
    ]
    
    # Cell ë§ˆìŠ¤í„° ë°ì´í„°
    cell_collection = db.cell_master
    cell_indexes = [
        IndexModel([("cell_id", ASCENDING)], unique=True, name="idx_cell_id_unique"),
        IndexModel([("peg_id", ASCENDING)], name="idx_cell_peg"),
        IndexModel([("cell_name", TEXT)], name="idx_cell_name_text")
    ]
    
    # PEG ì¸ë±ìŠ¤ ìƒì„±
    try:
        existing_indexes = await peg_collection.list_indexes().to_list(length=None)
        existing_names = {idx.get('name') for idx in existing_indexes}
        new_indexes = [idx for idx in peg_indexes if idx.document['name'] not in existing_names]
        
        if new_indexes:
            await peg_collection.create_indexes(new_indexes)
            logger.info(f"PEG ë§ˆìŠ¤í„° ì»¬ë ‰ì…˜ì— {len(new_indexes)}ê°œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        else:
            logger.info("PEG ë§ˆìŠ¤í„° ì»¬ë ‰ì…˜: ëª¨ë“  ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬")
    except Exception as e:
        logger.error(f"PEG ë§ˆìŠ¤í„° ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    
    # Cell ì¸ë±ìŠ¤ ìƒì„±
    try:
        existing_indexes = await cell_collection.list_indexes().to_list(length=None)
        existing_names = {idx.get('name') for idx in existing_indexes}
        new_indexes = [idx for idx in cell_indexes if idx.document['name'] not in existing_names]
        
        if new_indexes:
            await cell_collection.create_indexes(new_indexes)
            logger.info(f"Cell ë§ˆìŠ¤í„° ì»¬ë ‰ì…˜ì— {len(new_indexes)}ê°œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        else:
            logger.info("Cell ë§ˆìŠ¤í„° ì»¬ë ‰ì…˜: ëª¨ë“  ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬")
    except Exception as e:
        logger.error(f"Cell ë§ˆìŠ¤í„° ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")

async def optimize_all_collections(db: AsyncIOMotorDatabase):
    """
    ëª¨ë“  ì»¬ë ‰ì…˜ì— ìµœì í™”ëœ ì¸ë±ìŠ¤ ìƒì„±
    """
    logger.info("ğŸš€ ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™” ì‹œì‘...")
    
    try:
        # ê° ì»¬ë ‰ì…˜ë³„ ì¸ë±ìŠ¤ ìƒì„±
        await create_analysis_indexes(db)
        await create_preference_indexes(db)
        await create_statistics_indexes(db)
        await create_master_data_indexes(db)
        
        logger.info("âœ… ëª¨ë“  ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ")
        
        # ì¸ë±ìŠ¤ í†µê³„ ì¶œë ¥
        await print_index_statistics(db)
        
    except Exception as e:
        logger.error(f"âŒ ì¸ë±ìŠ¤ ìµœì í™” ì‹¤íŒ¨: {e}")
        raise

async def print_index_statistics(db: AsyncIOMotorDatabase):
    """
    ê° ì»¬ë ‰ì…˜ì˜ ì¸ë±ìŠ¤ í†µê³„ ì¶œë ¥
    """
    collections = [
        "analysis_results",
        "user_preferences", 
        "kpi_statistics",
        "peg_master",
        "cell_master"
    ]
    
    logger.info("ğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
    
    for collection_name in collections:
        try:
            collection = db[collection_name]
            indexes = await collection.list_indexes().to_list(length=None)
            index_count = len(indexes)
            
            logger.info(f"  {collection_name}: {index_count}ê°œ ì¸ë±ìŠ¤")
            
            for idx in indexes:
                name = idx.get('name', 'unnamed')
                key = idx.get('key', {})
                logger.debug(f"    - {name}: {dict(key)}")
                
        except Exception as e:
            logger.warning(f"  {collection_name}: í†µê³„ ì¡°íšŒ ì‹¤íŒ¨ - {e}")

async def analyze_query_performance(db: AsyncIOMotorDatabase, collection_name: str, query: Dict[str, Any]):
    """
    íŠ¹ì • ì¿¼ë¦¬ì˜ ì„±ëŠ¥ ë¶„ì„ (explain ì‚¬ìš©)
    """
    try:
        collection = db[collection_name]
        
        # ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ë¶„ì„
        explain_result = await collection.find(query).explain()
        
        execution_stats = explain_result.get('executionStats', {})
        
        analysis = {
            "query": query,
            "collection": collection_name,
            "execution_time_ms": execution_stats.get('executionTimeMillis', 0),
            "total_docs_examined": execution_stats.get('totalDocsExamined', 0),
            "total_docs_returned": execution_stats.get('totalDocsReturned', 0),
            "index_used": execution_stats.get('indexName'),
            "winning_plan": explain_result.get('queryPlanner', {}).get('winningPlan', {})
        }
        
        # ì„±ëŠ¥ í‰ê°€
        if analysis['total_docs_examined'] > analysis['total_docs_returned'] * 10:
            analysis['performance_warning'] = "ì¸ë±ìŠ¤ ìµœì í™” í•„ìš” - ë„ˆë¬´ ë§ì€ ë¬¸ì„œë¥¼ ê²€ì‚¬í•¨"
            
        return analysis
        
    except Exception as e:
        logger.error(f"ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}
