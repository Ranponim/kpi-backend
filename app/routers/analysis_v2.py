"""
ê°„ì†Œí™”ëœ ë¶„ì„ ê²°ê³¼ API ë¼ìš°í„° (v2)

í•µì‹¬ ëª©ì : MCP LLM ë¶„ì„ ê²°ê³¼ ì €ì¥ ë° í”„ë¡ íŠ¸ì—”ë“œ ì¡°íšŒ
ì„¤ê³„ ì›ì¹™: ë‹¨ìˆœì„±, ì„±ëŠ¥, ì‹¤ìš©ì„±
"""

import logging
from fastapi import APIRouter, Body, HTTPException, status, Query
from typing import Optional
from datetime import datetime
from pymongo.errors import PyMongoError
from bson import BSON

from ..db import get_database
from ..models.analysis_simplified import (
    AnalysisResultSimplifiedCreate,
    AnalysisResultSimplifiedModel,
    AnalysisResultSimplifiedResponse,
    AnalysisResultSimplifiedListResponse,
    AnalysisResultSimplifiedSummary,
    AnalysisResultSimplifiedFilter,
)
from ..models.common import PyObjectId
from ..exceptions import (
    AnalysisResultNotFoundException,
    InvalidAnalysisDataException,
    DatabaseConnectionException,
)

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger("app.analysis_v2")

# ë¼ìš°í„° ìƒì„±
router = APIRouter(
    prefix="/api/analysis/results-v2",
    tags=["Analysis Results V2 (Simplified)"],
    responses={
        400: {"description": "Bad Request"},
        404: {"description": "Not Found"},
        422: {"description": "Validation Error"},
        500: {"description": "Internal Server Error"}
    }
)


@router.post(
    "/",
    response_model=AnalysisResultSimplifiedResponse,
    response_model_by_alias=False,  # âœ… alias ëŒ€ì‹  í•„ë“œëª… ì‚¬ìš© (id í¬í•¨)
    status_code=status.HTTP_201_CREATED,
    summary="ë¶„ì„ ê²°ê³¼ ìƒì„± (ê°„ì†Œí™”)",
    description="MCPì—ì„œ ì „ì†¡í•œ LLM ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."
)
async def create_analysis_result_v2(
    result: AnalysisResultSimplifiedCreate = Body(
        ...,
        examples=[
            {
                "ne_id": "nvgnb#10000",
                "cell_id": "2010",
                "swname": "host01",
                "rel_ver": "R23A",
                "analysis_period": {
                    "n_minus_1_start": "2025-01-19 00:00:00",
                    "n_minus_1_end": "2025-01-19 23:59:59",
                    "n_start": "2025-01-20 00:00:00",
                    "n_end": "2025-01-20 23:59:59"
                },
                "choi_result": {
                    "enabled": True,
                    "status": "normal",
                    "score": 9.2
                },
                "llm_analysis": {
                    "executive_summary": "AirMacDLThruAvg(Kbps)ì˜ ê¸‰ê²©í•œ ê°ì†Œì™€ RandomlySelectedPreamblesLow(count)ì˜ í˜„ì €í•œ ë³€ë™ì„±ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.",
                    "diagnostic_findings": [
                        {
                            "primary_hypothesis": "ë‹¤ìš´ë§í¬ ìì› í• ë‹¹ ì‹¤íŒ¨ ë˜ëŠ” ì ‘ì† ì‹œë„ ì‹¤íŒ¨ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜",
                            "supporting_evidence": "AirMacDLThruAvg(Kbps)ì˜ ê·¹ì‹¬í•œ ê°ì†ŒëŠ” ë‹¤ìš´ë§í¬ ë°ì´í„° ì „ì†¡ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ...",
                            "confounding_factors_assessment": "ë™ì¼ í™˜ê²½ ê°€ì • í•˜ì—ì„œ, í•˜ë“œì›¨ì–´ ì˜¤ë¥˜ ê°€ëŠ¥ì„±ì€ ë‚®ë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤..."
                        }
                    ],
                    "recommended_actions": [
                        {
                            "priority": "P1",
                            "action": "ë‹¤ìš´ë§í¬ ìì› í• ë‹¹ ê´€ë ¨ ë¡œê·¸ ë¶„ì„ ë° ìŠ¤ì¼€ì¤„ë§ íŒŒë¼ë¯¸í„° í™•ì¸",
                            "details": "2025-09-04_12:30~2025-09-04_13:45 êµ¬ê°„ê³¼ 2025-09-05_12:45~2025-09-05_13:00 êµ¬ê°„ì˜ RRC connection setup failure ë¡œê·¸ë¥¼ ë¹„êµ ë¶„ì„..."
                        }
                    ],
                    "technical_analysis": "DL Throughput ê°ì†ŒëŠ” PRB í• ë‹¹ ì‹¤íŒ¨ì™€ ì§ì ‘ ì—°ê´€ë©ë‹ˆë‹¤.",
                    "cells_with_significant_change": ["cell_2010", "cell_2011"],
                    "action_plan": "1ë‹¨ê³„: ë¡œê·¸ ìˆ˜ì§‘ â†’ 2ë‹¨ê³„: RRC ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ â†’ 3ë‹¨ê³„: íŒŒë¼ë¯¸í„° ì¡°ì • â†’ 4ë‹¨ê³„: ëª¨ë‹ˆí„°ë§",
                    "key_findings": [
                        "DL Throughput 85% ê¸‰ê°",
                        "RACH Preamble ë³€ë™ì„± 200% ì¦ê°€",
                        "íŠ¹ì • ì‹œê°„ëŒ€ ì§‘ì¤‘ ë°œìƒ"
                    ],
                    "confidence": 0.92,
                    "model_name": "gemini-2.5-pro"
                },
                "peg_comparisons": [
                    {
                        "peg_name": "RACH_SUCCESS_RATE",
                        "n_minus_1": {"avg": 97.5, "pct_95": 99.0},
                        "n": {"avg": 98.2, "pct_95": 99.5},
                        "change_absolute": 0.7,
                        "change_percentage": 0.72
                    }
                ]
            },
            {
                "ne_id": "All NEs",
                "cell_id": "All cells",
                "swname": "All hosts",
                "rel_ver": None,
                "analysis_period": {
                    "n_minus_1_start": "2025-01-19 00:00:00",
                    "n_minus_1_end": "2025-01-19 23:59:59",
                    "n_start": "2025-01-20 00:00:00",
                    "n_end": "2025-01-20 23:59:59"
                },
                "llm_analysis": {
                    "summary": "ì „ì²´ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ - ëª¨ë“  ì…€ ì§‘ê³„",
                    "issues": ["ì¼ë¶€ ì…€ì—ì„œ ì„±ëŠ¥ ì €í•˜ ê°ì§€"],
                    "recommendations": ["ê°œë³„ ì…€ ë¶„ì„ ê¶Œì¥"],
                    "confidence": 0.85,
                    "model_name": "gemini-2.5-pro"
                },
                "peg_comparisons": [
                    {
                        "peg_name": "RACH_SUCCESS_RATE",
                        "n_minus_1": {"avg": 96.5, "pct_95": 98.5},
                        "n": {"avg": 97.0, "pct_95": 99.0},
                        "change_absolute": 0.5,
                        "change_percentage": 0.52
                    }
                ]
            }
        ]
    )
):
    """
    ìƒˆë¡œìš´ LLM ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    
    **í•„ìˆ˜ í•„ë“œ:**
    - ne_id: Network Element ID
    - cell_id: Cell Identity
    - swname: Software Name
    - analysis_period: ë¶„ì„ ê¸°ê°„ (N-1, N)
    - llm_analysis: LLM ë¶„ì„ ê²°ê³¼
    - peg_comparisons: PEG ë¹„êµ ê²°ê³¼
    
    **ì„ íƒ í•„ë“œ:**
    - rel_ver: Release Version
    - choi_result: Choi ì•Œê³ ë¦¬ì¦˜ ê²°ê³¼
    - analysis_id: ë¶„ì„ ê³ ìœ  ID
    """
    try:
        # ğŸ” 422 ì—ëŸ¬ ë””ë²„ê¹…: ìˆ˜ì‹ ëœ ë°ì´í„° êµ¬ì¡° ë¡œê¹…
        logger.debug("ğŸ” [422 ë””ë²„ê¹…] ìˆ˜ì‹ ëœ ìš”ì²­ ë°ì´í„°:")
        logger.debug("  - result type: %s", type(result).__name__)
        logger.debug("  - result dict keys: %s", list(result.model_dump().keys()) if hasattr(result, 'model_dump') else "No model_dump method")
        
        # LLM ë¶„ì„ ë°ì´í„° ìƒì„¸ ë¡œê¹…
        if hasattr(result, 'llm_analysis') and result.llm_analysis:
            llm_data = result.llm_analysis
            logger.debug("  - llm_analysis type: %s", type(llm_data).__name__)
            logger.debug("  - llm_analysis keys: %s", list(llm_data.model_dump().keys()) if hasattr(llm_data, 'model_dump') else "No model_dump method")
            
            # diagnostic_findings êµ¬ì¡° í™•ì¸
            if hasattr(llm_data, 'diagnostic_findings'):
                findings = llm_data.diagnostic_findings
                logger.debug("  - diagnostic_findings: %dê°œ", len(findings) if findings else 0)
                if findings and len(findings) > 0:
                    first_finding = findings[0]
                    logger.debug("  - ì²« ë²ˆì§¸ finding type: %s", type(first_finding).__name__)
                    logger.debug("  - ì²« ë²ˆì§¸ finding keys: %s", list(first_finding.model_dump().keys()) if hasattr(first_finding, 'model_dump') else "No model_dump method")
            
            # recommended_actions êµ¬ì¡° í™•ì¸
            if hasattr(llm_data, 'recommended_actions'):
                actions = llm_data.recommended_actions
                logger.debug("  - recommended_actions: %dê°œ", len(actions) if actions else 0)
                if actions and len(actions) > 0:
                    first_action = actions[0]
                    logger.debug("  - ì²« ë²ˆì§¸ action type: %s", type(first_action).__name__)
                    logger.debug("  - ì²« ë²ˆì§¸ action keys: %s", list(first_action.model_dump().keys()) if hasattr(first_action, 'model_dump') else "No model_dump method")
        
        db = get_database()
        collection = db.analysis_results_v2
        
        # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜
        payload = result.model_dump(by_alias=False, exclude_unset=True)
        
        # created_at ìë™ ì„¤ì •
        payload["created_at"] = datetime.utcnow()
        
        logger.info(
            "ë¶„ì„ ê²°ê³¼ ìƒì„± ì‹œë„: ne_id=%s, cell_id=%s, swname=%s",
            result.ne_id,
            result.cell_id,
            result.swname
        )
        
        # MongoDB ë¬¸ì„œ í¬ê¸° ì²´í¬ (16MB ì œí•œ)
        try:
            encoded = BSON.encode(payload)
            doc_size = len(encoded)
            max_size = 16 * 1024 * 1024
            
            if doc_size > max_size:
                logger.error(f"ë¬¸ì„œ í¬ê¸° ì´ˆê³¼: {doc_size}B > 16MB")
                raise InvalidAnalysisDataException(
                    f"Document too large ({doc_size} bytes). Reduce PEG count or details."
                )
            
            logger.debug(f"ë¬¸ì„œ í¬ê¸°: {doc_size / 1024:.2f} KB")
            
        except Exception as e:
            logger.warning(f"ë¬¸ì„œ í¬ê¸° ì²´í¬ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        
        # ì¤‘ë³µ ê²€ì‚¬ (ê°™ì€ NE, Cell, swname, ë¹„ìŠ·í•œ ì‹œê°„ì˜ ê²°ê³¼ ë°©ì§€)
        # ìµœê·¼ 1ë¶„ ì´ë‚´ì— ë™ì¼í•œ ì¡°í•©ì˜ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
        one_minute_ago = datetime.utcnow().timestamp() - 60
        existing = await collection.find_one({
            "ne_id": result.ne_id,
            "cell_id": result.cell_id,
            "swname": result.swname,
            "created_at": {"$gte": datetime.fromtimestamp(one_minute_ago)}
        })
        
        if existing:
            logger.warning(
                "ìµœê·¼ ì¤‘ë³µ ê²°ê³¼ ë°œê²¬: ne_id=%s, cell_id=%s, swname=%s",
                result.ne_id,
                result.cell_id,
                result.swname
            )
            # ì¤‘ë³µì´ì–´ë„ ì €ì¥ì€ í—ˆìš© (warningë§Œ ì¶œë ¥)
        
        # ë¬¸ì„œ ì‚½ì…
        insert_result = await collection.insert_one(payload)
        
        # ìƒì„±ëœ ë¬¸ì„œ ì¡°íšŒ
        created_doc = await collection.find_one({"_id": insert_result.inserted_id})
        
        if not created_doc:
            raise DatabaseConnectionException("Failed to retrieve created analysis result")
        
        # ì‘ë‹µ ëª¨ë¸ë¡œ ë³€í™˜
        analysis_model = AnalysisResultSimplifiedModel.from_mongo(created_doc)
        
        logger.info(
            "ë¶„ì„ ê²°ê³¼ ìƒì„± ì™„ë£Œ: id=%s, ne_id=%s, cell_id=%s, swname=%s",
            str(insert_result.inserted_id),
            result.ne_id,
            result.cell_id,
            result.swname
        )
        
        return AnalysisResultSimplifiedResponse(
            message="Analysis result created successfully",
            data=analysis_model
        )
        
    except PyMongoError as e:
        logger.error(f"MongoDB ì˜¤ë¥˜: {e}", exc_info=True)
        raise DatabaseConnectionException(f"Database operation failed: {str(e)}")
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise InvalidAnalysisDataException(f"Failed to create analysis result: {str(e)}")


@router.get(
    "/",
    response_model=AnalysisResultSimplifiedListResponse,
    response_model_by_alias=False,  # âœ… alias ëŒ€ì‹  í•„ë“œëª… ì‚¬ìš© (id í¬í•¨)
    summary="ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ",
    description="í•„í„°ë§ ë° í˜ì´ì§€ë„¤ì´ì…˜ì„ ì§€ì›í•˜ëŠ” ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ"
)
async def list_analysis_results_v2(
    page: int = Query(1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸"),
    size: int = Query(20, ge=1, le=100, description="í˜ì´ì§€ í¬ê¸°"),
    ne_id: Optional[str] = Query(None, description="NE ID í•„í„°"),
    cell_id: Optional[str] = Query(None, description="Cell ID í•„í„°"),
    swname: Optional[str] = Query(None, description="SW Name í•„í„°"),
    rel_ver: Optional[str] = Query(None, description="Release Version í•„í„°"),
    date_from: Optional[datetime] = Query(None, description="ì‹œì‘ ë‚ ì§œ"),
    date_to: Optional[datetime] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ"),
    choi_status: Optional[str] = Query(None, description="Choi íŒì • ìƒíƒœ"),
):
    """
    ë¶„ì„ ê²°ê³¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    **í•„í„° ì˜µì…˜:**
    - ne_id, cell_id, swname: ì‹ë³„ì ê¸°ë°˜ í•„í„°ë§
    - rel_ver: Release ë²„ì „ í•„í„°ë§
    - date_from, date_to: ì‹œê°„ ë²”ìœ„ í•„í„°ë§
    - choi_status: Choi ì•Œê³ ë¦¬ì¦˜ íŒì • ê²°ê³¼ í•„í„°ë§
    
    **ì •ë ¬:** created_at ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ (ìµœì‹ ìˆœ)
    """
    try:
        db = get_database()
        collection = db.analysis_results_v2
        
        # í•„í„° ì¡°ê±´ êµ¬ì„± (ë³µí•© ì¸ë±ìŠ¤ í™œìš© ìˆœì„œ: ne_id â†’ cell_id â†’ swname)
        filter_dict = {}
        
        if ne_id:
            filter_dict["ne_id"] = ne_id
        if cell_id:
            filter_dict["cell_id"] = cell_id
        if swname:
            filter_dict["swname"] = swname
        if rel_ver:
            filter_dict["rel_ver"] = rel_ver
        if choi_status:
            filter_dict["choi_result.status"] = choi_status
            
        if date_from or date_to:
            filter_dict["created_at"] = {}
            if date_from:
                filter_dict["created_at"]["$gte"] = date_from
            if date_to:
                filter_dict["created_at"]["$lte"] = date_to
        
        logger.info(
            "ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ: page=%d, size=%d, filters=%s",
            page,
            size,
            filter_dict
        )
        
        # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
        total_count = await collection.count_documents(filter_dict)
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ê³„ì‚°
        skip = (page - 1) * size
        has_next = (skip + size) < total_count
        
        # ë°ì´í„° ì¡°íšŒ
        cursor = collection.find(filter_dict)
        cursor = cursor.sort("created_at", -1).skip(skip).limit(size)
        
        documents = await cursor.to_list(length=size)
        
        # ì‘ë‹µ ëª¨ë¸ë¡œ ë³€í™˜
        items = [AnalysisResultSimplifiedModel.from_mongo(doc) for doc in documents]
        
        logger.info(
            "ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ ì™„ë£Œ: total=%d, returned=%d, page=%d",
            total_count,
            len(items),
            page
        )
        
        return AnalysisResultSimplifiedListResponse(
            items=items,
            total=total_count,
            page=page,
            size=size,
            has_next=has_next
        )
        
    except PyMongoError as e:
        logger.error(f"MongoDB ì˜¤ë¥˜: {e}", exc_info=True)
        raise DatabaseConnectionException(f"Database query failed: {str(e)}")
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise DatabaseConnectionException(f"Failed to retrieve analysis results: {str(e)}")


@router.get(
    "/{result_id}",
    response_model=AnalysisResultSimplifiedResponse,
    response_model_by_alias=False,  # âœ… alias ëŒ€ì‹  í•„ë“œëª… ì‚¬ìš© (id í¬í•¨)
    summary="ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì¡°íšŒ",
    description="íŠ¹ì • IDì˜ ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì •ë³´ ì¡°íšŒ"
)
async def get_analysis_result_v2(result_id: PyObjectId):
    """
    íŠ¹ì • IDì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„¸ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    **ë°˜í™˜ ì •ë³´:**
    - ì „ì²´ PEG ë¹„êµ ê²°ê³¼
    - LLM ë¶„ì„ ìƒì„¸
    - Choi ì•Œê³ ë¦¬ì¦˜ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
    """
    try:
        db = get_database()
        collection = db.analysis_results_v2
        
        logger.info("ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì¡°íšŒ: id=%s", str(result_id))
        
        # ë¬¸ì„œ ì¡°íšŒ
        document = await collection.find_one({"_id": result_id})
        
        if not document:
            raise AnalysisResultNotFoundException(str(result_id))
        
        # ì‘ë‹µ ëª¨ë¸ë¡œ ë³€í™˜
        analysis_model = AnalysisResultSimplifiedModel.from_mongo(document)
        
        logger.info(
            "ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì¡°íšŒ ì™„ë£Œ: id=%s, ne_id=%s, cell_id=%s",
            str(result_id),
            analysis_model.ne_id,
            analysis_model.cell_id
        )
        
        return AnalysisResultSimplifiedResponse(
            message="Analysis result retrieved successfully",
            data=analysis_model
        )
        
    except AnalysisResultNotFoundException:
        raise
        
    except PyMongoError as e:
        logger.error(f"MongoDB ì˜¤ë¥˜: {e}", exc_info=True)
        raise DatabaseConnectionException(f"Database query failed: {str(e)}")
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise DatabaseConnectionException(f"Failed to retrieve analysis result: {str(e)}")


@router.delete(
    "/{result_id}",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="ë¶„ì„ ê²°ê³¼ ì‚­ì œ",
    description="íŠ¹ì • IDì˜ ë¶„ì„ ê²°ê³¼ ì‚­ì œ"
)
async def delete_analysis_result_v2(result_id: PyObjectId):
    """
    íŠ¹ì • IDì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    try:
        db = get_database()
        collection = db.analysis_results_v2
        
        logger.info("ë¶„ì„ ê²°ê³¼ ì‚­ì œ ì‹œë„: id=%s", str(result_id))
        
        # ë¬¸ì„œ ì‚­ì œ
        delete_result = await collection.delete_one({"_id": result_id})
        
        if delete_result.deleted_count == 0:
            raise AnalysisResultNotFoundException(str(result_id))
        
        logger.info("ë¶„ì„ ê²°ê³¼ ì‚­ì œ ì™„ë£Œ: id=%s", str(result_id))
        
        # 204 No Content ì‘ë‹µ
        return
        
    except AnalysisResultNotFoundException:
        raise
        
    except PyMongoError as e:
        logger.error(f"MongoDB ì˜¤ë¥˜: {e}", exc_info=True)
        raise DatabaseConnectionException(f"Database delete failed: {str(e)}")
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise DatabaseConnectionException(f"Failed to delete analysis result: {str(e)}")


@router.get(
    "/stats/summary",
    summary="í†µê³„ ìš”ì•½",
    description="ë¶„ì„ ê²°ê³¼ í†µê³„ ìš”ì•½ (NE, Cell, swnameë³„ ì§‘ê³„)"
)
async def get_analysis_stats_v2():
    """
    ë¶„ì„ ê²°ê³¼ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    **ë°˜í™˜ ì •ë³´:**
    - ì „ì²´ ë¶„ì„ ê±´ìˆ˜
    - NEë³„ ë¶„ì„ ê±´ìˆ˜
    - Cellë³„ ë¶„ì„ ê±´ìˆ˜
    - Choi íŒì • ë¶„í¬
    - ìµœê·¼ ë¶„ì„ ê²°ê³¼ (10ê±´)
    """
    try:
        db = get_database()
        collection = db.analysis_results_v2
        
        logger.info("ë¶„ì„ í†µê³„ ìš”ì•½ ì¡°íšŒ")
        
        # ì „ì²´ ê°œìˆ˜
        total_count = await collection.count_documents({})
        
        # NEë³„ ì§‘ê³„
        ne_stats = await collection.aggregate([
            {"$group": {"_id": "$ne_id", "count": {"$sum": 1}}},
            {"$sort": {"count": -1}},
            {"$limit": 10}
        ]).to_list(length=10)
        
        # Choi íŒì • ë¶„í¬
        choi_stats = await collection.aggregate([
            {"$match": {"choi_result.enabled": True}},
            {"$group": {"_id": "$choi_result.status", "count": {"$sum": 1}}}
        ]).to_list(length=None)
        
        # ìµœê·¼ ë¶„ì„ ê²°ê³¼
        recent_results = await collection.find(
            {},
            {"ne_id": 1, "cell_id": 1, "swname": 1, "created_at": 1, "llm_analysis.summary": 1}
        ).sort("created_at", -1).limit(10).to_list(length=10)
        
        summary = {
            "total_count": total_count,
            "by_ne": {stat["_id"]: stat["count"] for stat in ne_stats},
            "choi_distribution": {stat["_id"]: stat["count"] for stat in choi_stats},
            "recent_results": recent_results
        }
        
        logger.info("ë¶„ì„ í†µê³„ ìš”ì•½ ì¡°íšŒ ì™„ë£Œ: total=%d", total_count)
        
        return {
            "success": True,
            "data": summary
        }
        
    except PyMongoError as e:
        logger.error(f"MongoDB ì˜¤ë¥˜: {e}", exc_info=True)
        raise DatabaseConnectionException(f"Database aggregation failed: {str(e)}")
        
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise DatabaseConnectionException(f"Failed to retrieve statistics: {str(e)}")


# ë¼ìš°í„° export
__all__ = ["router"]




