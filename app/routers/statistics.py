"""
Statistics ë¹„êµ ë¶„ì„ API ë¼ìš°í„°

ì´ ëª¨ë“ˆì€ ë‘ ê°œì˜ ë‚ ì§œ êµ¬ê°„ì— ëŒ€í•œ KPI ë°ì´í„° ë¹„êµ ë¶„ì„ì„ ìœ„í•œ
FastAPI ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

from datetime import datetime
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, HTTPException, Depends, status, BackgroundTasks
from fastapi.responses import JSONResponse
import logging

from ..db import get_database
from ..models.statistics import (
    StatisticsCompareRequest, StatisticsCompareResponse, StatisticsCompareError,
    PegComparisonResult
)
from ..utils.statistics_db import StatisticsDataBase
from ..utils.statistics_analyzer import StatisticsAnalyzer, validate_data_consistency
from ..exceptions import DatabaseConnectionException, InvalidAnalysisDataException

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ë¼ìš°í„° ìƒì„±
router = APIRouter(
    prefix="/api/statistics",
    tags=["statistics"],
    responses={
        500: {"model": StatisticsCompareError, "description": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜"},
        400: {"model": StatisticsCompareError, "description": "ì˜ëª»ëœ ìš”ì²­"},
        404: {"model": StatisticsCompareError, "description": "ë°ì´í„° ì—†ìŒ"}
    }
)

async def get_statistics_db(db=Depends(get_database)) -> StatisticsDataBase:
    """Statistics ë°ì´í„°ë² ì´ìŠ¤ ì˜ì¡´ì„± ì£¼ì…"""
    return StatisticsDataBase(db)

@router.post(
    "/compare",
    response_model=StatisticsCompareResponse,
    summary="ë‘ ê¸°ê°„ KPI ë°ì´í„° ë¹„êµ ë¶„ì„",
    description="""
    ë‘ ê°œì˜ ë‚ ì§œ êµ¬ê°„ì— ëŒ€í•œ KPI ë°ì´í„°ë¥¼ ë¹„êµ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì§€í‘œë“¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤:
    
    **ë¶„ì„ ì§€í‘œ:**
    - í‰ê· ê°’ (Mean)
    - ë¸íƒ€ (Delta): period2 - period1
    - ë¸íƒ€ ë°±ë¶„ìœ¨ (Delta Percentage)
    - RSD (Relative Standard Deviation): ìƒëŒ€í‘œì¤€í¸ì°¨
    - í†µê³„ì  ìœ ì˜ì„± (t-ê²€ì •)
    - ê°œì„ /ì•…í™” ìƒíƒœ íŒì •
    
    **í•„í„° ì˜µì…˜:**
    - NE í•„í„°ë§
    - Cell ID í•„í„°ë§  
    - ì´ìƒì¹˜ í¬í•¨/ì œì™¸
    - ì†Œìˆ˜ì  ìë¦¿ìˆ˜ ì„¤ì •
    """,
    responses={
        200: {
            "description": "ë¹„êµ ë¶„ì„ ì„±ê³µ",
            "content": {
                "application/json": {
                    "example": {
                        "request_summary": {
                            "period1": "2025-08-01 to 2025-08-07",
                            "period2": "2025-08-08 to 2025-08-14",
                            "peg_count": 3
                        },
                        "analysis_results": [
                            {
                                "peg_name": "availability",
                                "delta": 0.2,
                                "delta_percentage": 0.201,
                                "improvement_status": "improved"
                            }
                        ],
                        "summary": {
                            "total_pegs_analyzed": 3,
                            "improved_count": 2,
                            "degraded_count": 1
                        }
                    }
                }
            }
        }
    }
)
async def compare_statistics(
    request: StatisticsCompareRequest,
    background_tasks: BackgroundTasks,
    stats_db: StatisticsDataBase = Depends(get_statistics_db)
) -> StatisticsCompareResponse:
    """
    ë‘ ê¸°ê°„ì˜ KPI ë°ì´í„° ë¹„êµ ë¶„ì„ API
    
    Args:
        request: ë¹„êµ ë¶„ì„ ìš”ì²­ ë°ì´í„°
        background_tasks: ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… (ë¡œê¹…, ìºì‹± ë“±)
        stats_db: Statistics ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        ë¹„êµ ë¶„ì„ ê²°ê³¼
        
    Raises:
        HTTPException: ìš”ì²­ ì˜¤ë¥˜, ë°ì´í„° ë¶€ì¡±, ì„œë²„ ì˜¤ë¥˜ ë“±
    """
    start_time = datetime.utcnow()
    
    try:
        logger.info("Statistics ë¹„êµ ë¶„ì„ ìš”ì²­ ì‹œì‘")
        logger.info(f"ìš”ì²­ íŒŒë¼ë¯¸í„°: period1={request.period1.start_date}~{request.period1.end_date}, "
                   f"period2={request.period2.start_date}~{request.period2.end_date}, "
                   f"PEGs={request.peg_names}")
        
        # 1. ë°ì´í„° ê°€ìš©ì„± ê²€ì¦
        await validate_request_data_availability(request, stats_db)
        
        # 2. ë‘ ê¸°ê°„ ë°ì´í„° ì¡°íšŒ
        period1_data, period2_data = await fetch_comparison_data(request, stats_db)
        
        # 3. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦  
        validation_result = validate_data_consistency(period1_data, period2_data, request.peg_names)
        
        if not validation_result["is_consistent"]:
            logger.warning(f"ë°ì´í„° ì¼ê´€ì„± ë¬¸ì œ: {validation_result}")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ PEGë§Œìœ¼ë¡œ ë¶„ì„ ì§„í–‰
            available_pegs = validation_result["available_pegs"]
            if not available_pegs:
                raise InvalidAnalysisDataException(
                    "ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê³µí†µ PEG ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤",
                    validation_errors=[validation_result]
                )
            
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ PEGë¡œ ë¶„ì„ ì§„í–‰: {available_pegs}")
            request.peg_names = available_pegs
        
        # 4. í†µê³„ ë¶„ì„ ìˆ˜í–‰
        analyzer = StatisticsAnalyzer(decimal_places=request.decimal_places)
        comparison_results = analyzer.calculate_comparison_statistics(
            period1_data, period2_data, request.peg_names
        )
        
        if not comparison_results:
            raise InvalidAnalysisDataException(
                "ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                validation_errors=["insufficient_data", validation_result]
            )
        
        # 5. ìš”ì•½ í†µê³„ ê³„ì‚°
        summary_stats = analyzer.calculate_summary_statistics(comparison_results)
        
        # 6. ì‘ë‹µ ìƒì„±
        response = create_comparison_response(
            request, comparison_results, summary_stats, 
            analyzer, start_time, validation_result
        )
        
        # 7. ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ë“±ë¡ (ë¡œê¹…, ë©”íŠ¸ë¦­ ë“±)
        background_tasks.add_task(
            log_analysis_metrics, 
            request, len(comparison_results), start_time
        )
        
        logger.info(f"Statistics ë¹„êµ ë¶„ì„ ì™„ë£Œ - PEG ìˆ˜: {len(comparison_results)}")
        return response
        
    except InvalidAnalysisDataException as e:
        logger.error(f"ìš”ì²­ ê²€ì¦ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=StatisticsCompareError(
                error_code="VALIDATION_ERROR",
                error_message=str(e),
                details=getattr(e, 'details', None),
                suggestions=[
                    "ë‚ ì§œ ë²”ìœ„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
                    "PEG ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”", 
                    "í•„í„° ì¡°ê±´ì„ ì™„í™”í•´ë³´ì„¸ìš”"
                ]
            ).dict()
        )
        
    except DatabaseConnectionException as e:
        logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=StatisticsCompareError(
                error_code="DATABASE_ERROR",
                error_message="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                details={"original_error": str(e)},
                suggestions=["ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”"]
            ).dict()
        )
        
    except Exception as e:
        logger.error(f"Statistics ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=StatisticsCompareError(
                error_code="INTERNAL_ERROR",
                error_message="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                details={"error_type": type(e).__name__},
                suggestions=["ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”"]
            ).dict()
        )

async def validate_request_data_availability(
    request: StatisticsCompareRequest,
    stats_db: StatisticsDataBase
) -> None:
    """
    ìš”ì²­ëœ ë°ì´í„°ì˜ ê°€ìš©ì„±ì„ ì‚¬ì „ ê²€ì¦
    
    Args:
        request: ë¶„ì„ ìš”ì²­
        stats_db: ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        
    Raises:
        InvalidAnalysisDataException: ë°ì´í„° ë¶€ì¡± ì‹œ
    """
    logger.info("ë°ì´í„° ê°€ìš©ì„± ì‚¬ì „ ê²€ì¦ ì‹œì‘")
    
    # ê° ê¸°ê°„ë³„ ë°ì´í„° ê°€ìš©ì„± í™•ì¸
    period1_availability = await stats_db.validate_data_availability(
        request.period1.start_date, request.period1.end_date, request.peg_names,
        request.ne_filter, request.cell_id_filter
    )
    
    period2_availability = await stats_db.validate_data_availability(
        request.period2.start_date, request.period2.end_date, request.peg_names,
        request.ne_filter, request.cell_id_filter
    )
    
    # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ í™•ì¸
    if period1_availability["total_data_points"] == 0:
        raise InvalidAnalysisDataException(
            "ì²« ë²ˆì§¸ ê¸°ê°„ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤",
            validation_errors=[period1_availability, request.peg_names]
        )
    
    if period2_availability["total_data_points"] == 0:
        raise InvalidAnalysisDataException(
            "ë‘ ë²ˆì§¸ ê¸°ê°„ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤",
            validation_errors=[period2_availability, request.peg_names]
        )
    
    # ê³µí†µ PEG í™•ì¸
    common_pegs = set(period1_availability["available_pegs"]) & set(period2_availability["available_pegs"])
    if not common_pegs:
        raise InvalidAnalysisDataException(
            "ë‘ ê¸°ê°„ ëª¨ë‘ì— ì¡´ì¬í•˜ëŠ” ê³µí†µ PEGê°€ ì—†ìŠµë‹ˆë‹¤",
            validation_errors=[
                period1_availability["available_pegs"],
                period2_availability["available_pegs"],
                request.peg_names
            ]
        )
    
    logger.info(f"ë°ì´í„° ê°€ìš©ì„± ê²€ì¦ ì™„ë£Œ - ê³µí†µ PEG: {len(common_pegs)}ê°œ")

async def fetch_comparison_data(
    request: StatisticsCompareRequest,
    stats_db: StatisticsDataBase
) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    ë¹„êµ ë¶„ì„ì„ ìœ„í•œ ë‘ ê¸°ê°„ ë°ì´í„° ì¡°íšŒ
    
    Args:
        request: ë¶„ì„ ìš”ì²­
        stats_db: ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        (period1_data, period2_data) íŠœí”Œ
    """
    logger.info("ë¹„êµ ë°ì´í„° ì¡°íšŒ ì‹œì‘")
    
    # ë³‘ë ¬ë¡œ ë‘ ê¸°ê°„ ë°ì´í„° ì¡°íšŒ
    period1_data = await stats_db.get_period_data(
        request.period1.start_date, request.period1.end_date,
        request.peg_names, request.ne_filter, request.cell_id_filter,
        request.include_outliers
    )
    
    period2_data = await stats_db.get_period_data(
        request.period2.start_date, request.period2.end_date,
        request.peg_names, request.ne_filter, request.cell_id_filter,
        request.include_outliers
    )
    
    logger.info(f"ë°ì´í„° ì¡°íšŒ ì™„ë£Œ - Period1: {len(period1_data)}ê°œ, Period2: {len(period2_data)}ê°œ")
    return period1_data, period2_data

def create_comparison_response(
    request: StatisticsCompareRequest,
    comparison_results: List[PegComparisonResult],
    summary_stats: Dict[str, Any],
    analyzer: StatisticsAnalyzer,
    start_time: datetime,
    validation_result: Dict[str, Any]
) -> StatisticsCompareResponse:
    """
    ë¹„êµ ë¶„ì„ ì‘ë‹µ ìƒì„±
    
    Args:
        request: ì›ë³¸ ìš”ì²­
        comparison_results: ë¹„êµ ë¶„ì„ ê²°ê³¼
        summary_stats: ìš”ì•½ í†µê³„
        analyzer: ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
        start_time: ë¶„ì„ ì‹œì‘ ì‹œê°„
        validation_result: ë°ì´í„° ê²€ì¦ ê²°ê³¼
        
    Returns:
        ì™„ì„±ëœ ì‘ë‹µ ê°ì²´
    """
    logger.info("ë¹„êµ ë¶„ì„ ì‘ë‹µ ìƒì„±")
    
    # ìš”ì²­ ìš”ì•½ ìƒì„±
    request_summary = {
        "period1": f"{request.period1.start_date.strftime('%Y-%m-%d')} to {request.period1.end_date.strftime('%Y-%m-%d')}",
        "period2": f"{request.period2.start_date.strftime('%Y-%m-%d')} to {request.period2.end_date.strftime('%Y-%m-%d')}",
        "peg_count": len(request.peg_names),
        "requested_pegs": request.peg_names,
        "filter_applied": bool(request.ne_filter or request.cell_id_filter),
        "ne_filter": request.ne_filter,
        "cell_id_filter": request.cell_id_filter,
        "include_outliers": request.include_outliers,
        "decimal_places": request.decimal_places
    }
    
    # í•„í„° ì •ë³´
    filters_applied = {
        "ne_filter": request.ne_filter,
        "cell_id_filter": request.cell_id_filter,
        "include_outliers": request.include_outliers
    }
    
    # ë©”íƒ€ë°ì´í„° ìƒì„±
    metadata = analyzer.generate_analysis_metadata(
        start_time, 
        validation_result["period1_data_count"],
        validation_result["period2_data_count"],
        request.peg_names,
        filters_applied
    )
    
    # ì‘ë‹µ ê°ì²´ ìƒì„±
    response = StatisticsCompareResponse(
        request_summary=request_summary,
        analysis_results=comparison_results,
        summary=summary_stats,
        metadata=metadata
    )
    
    return response

async def log_analysis_metrics(
    request: StatisticsCompareRequest,
    results_count: int,
    start_time: datetime
) -> None:
    """
    ë¶„ì„ ë©”íŠ¸ë¦­ ë¡œê¹… (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)
    
    Args:
        request: ë¶„ì„ ìš”ì²­
        results_count: ê²°ê³¼ ìˆ˜
        start_time: ì‹œì‘ ì‹œê°„
    """
    try:
        end_time = datetime.utcnow()
        processing_time = (end_time - start_time).total_seconds()
        
        logger.info(f"ğŸ“Š Statistics ë¶„ì„ ë©”íŠ¸ë¦­:")
        logger.info(f"  - ì²˜ë¦¬ ì‹œê°„: {processing_time:.3f}ì´ˆ")
        logger.info(f"  - ë¶„ì„ëœ PEG: {results_count}ê°œ")
        logger.info(f"  - ìš”ì²­ëœ PEG: {len(request.peg_names)}ê°œ")
        logger.info(f"  - í•„í„° ì ìš©: {bool(request.ne_filter or request.cell_id_filter)}")
        logger.info(f"  - ì´ìƒì¹˜ í¬í•¨: {request.include_outliers}")
        
        # í–¥í›„ ë©”íŠ¸ë¦­ ì €ì¥ì†Œ(ì˜ˆ: InfluxDB, Prometheus)ì— ì „ì†¡ ê°€ëŠ¥
        
    except Exception as e:
        logger.error(f"ë©”íŠ¸ë¦­ ë¡œê¹… ì‹¤íŒ¨: {e}")

@router.get(
    "/health",
    summary="Statistics API ìƒíƒœ í™•ì¸",
    description="Statistics ë¹„êµ ë¶„ì„ APIì˜ ìƒíƒœì™€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í™•ì¸í•©ë‹ˆë‹¤.",
    response_model=Dict[str, Any]
)
async def statistics_health_check(
    stats_db: StatisticsDataBase = Depends(get_statistics_db)
) -> Dict[str, Any]:
    """
    Statistics API ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
    
    Returns:
        API ìƒíƒœ ì •ë³´
    """
    try:
        logger.info("Statistics API ìƒíƒœ í™•ì¸ ì‹œì‘")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
        test_start = datetime(2025, 8, 1)
        test_end = datetime(2025, 8, 2)
        availability = await stats_db.validate_data_availability(
            test_start, test_end, ['availability']
        )
        
        return {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "database_connection": "ok",
            "available_data_points": availability.get("total_data_points", 0),
            "api_version": "1.0.0"
        }
        
    except Exception as e:
        logger.error(f"Statistics API ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return {
            "status": "unhealthy", 
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "database_connection": "failed",
            "error": str(e),
            "api_version": "1.0.0"
        }

# Mock ë°ì´í„° ìƒì„± ì—”ë“œí¬ì¸íŠ¸ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
@router.post(
    "/mock-data",
    summary="Mock ë°ì´í„° ìƒì„± (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)",
    description="Statistics ë¶„ì„ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ Mock ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=Dict[str, Any]
)
async def generate_mock_data(
    count: int = 1000,
    stats_db: StatisticsDataBase = Depends(get_statistics_db)
) -> Dict[str, Any]:
    """
    Mock ë°ì´í„° ìƒì„± ì—”ë“œí¬ì¸íŠ¸ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
    
    Args:
        count: ìƒì„±í•  ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜
        stats_db: ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        ìƒì„± ê²°ê³¼
    """
    try:
        logger.info(f"Mock ë°ì´í„° ìƒì„± ì‹œì‘: {count}ê°œ")
        
        from ..utils.statistics_db import create_sample_data
        await create_sample_data(stats_db.db, count)
        
        return {
            "status": "success",
            "message": f"Mock ë°ì´í„° {count}ê°œ ìƒì„± ì™„ë£Œ",
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        
    except Exception as e:
        logger.error(f"Mock ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Mock ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {str(e)}"
        )
